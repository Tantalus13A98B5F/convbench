{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topi test\n",
    "\n",
    "test the correctness of the sparse operator\n",
    "\n",
    "implementation listed in the TOPI (tvm operator inventory)\n",
    "\n",
    "tvm basics:\n",
    "\n",
    "1. create compute scheme (e.g. `topi.x86.sparse.spconv2d_3x3_nhwc`)\n",
    "2. create schedule to describe the implementation details of the compute (e.g. `topi.x86.sparse.schedule_spconv2d_3x3_nhwc`)\n",
    "3. if there are tunable parameters in the schedule, they can be searched (known as tuning, by tvm.autotvm)\n",
    "4. build the compute with the schedule (filled with tuning results)\n",
    "\n",
    "when you see `llvm -mcpu=cascadelake`, that's my machine config. Look for the `-mcpu` arg for your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import topi, testing, nd, target, te\n",
    "from sparse_utils import random_bsr_sparse\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "N, C, HW, VL, SP = 10, 64, 56, 16, 0.5\n",
    "NNZ = int(C * C * 9 // VL * SP)\n",
    "spweight = random_bsr_sparse((C, 9*C), (VL, 1), NNZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -keys=cpu -link-params=0 -mcpu=cascadelake, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1152, 16, 1), 'float32'), ('TENSOR', (1152,), 'int32'), ('TENSOR', (5,), 'int32')). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHWC passed\n"
     ]
    }
   ],
   "source": [
    "# NHWC\n",
    "\n",
    "data = np.random.rand(N, HW, HW, C).astype('float32')\n",
    "weight = spweight.toarray().T.copy().reshape(3, 3, C, C)\n",
    "Data = te.placeholder(data.shape, 'float32')\n",
    "\n",
    "## dense\n",
    "\n",
    "Weight = te.placeholder(weight.shape, 'float32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.nn.conv2d_nhwc(Data, Weight, 1, 1, 1)\n",
    "    s = topi.generic.schedule_conv2d_nhwc(CC)\n",
    "    func = tvm.build(s, [Data, Weight, CC])\n",
    "args = [nd.array(data), nd.array(weight), nd.empty(CC.shape)]\n",
    "func(*args)\n",
    "\n",
    "## sparse\n",
    "\n",
    "Wdat = te.placeholder(spweight.data.shape, 'float32')\n",
    "Wind = te.placeholder(spweight.indices.shape, 'int32')\n",
    "Wptr = te.placeholder(spweight.indptr.shape, 'int32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.x86.sparse.spconv2d_3x3_nhwc(Data, Wdat, Wind, Wptr)\n",
    "    s = topi.x86.sparse.schedule_spconv2d_3x3_nhwc(CC)\n",
    "    func = tvm.build(s, [Data, Wdat, Wind, Wptr, CC])\n",
    "args2 = [nd.array(data), nd.array(spweight.data), nd.array(spweight.indices), nd.array(spweight.indptr), nd.empty(CC.shape)]\n",
    "func(*args2)\n",
    "\n",
    "## assert\n",
    "\n",
    "testing.assert_allclose(args[-1].numpy(), args2[-1].numpy())\n",
    "print('NHWC passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -keys=cpu -link-params=0 -mcpu=cascadelake, workload=('conv3x3_spNCHW.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (1152, 16, 1), 'float32'), ('TENSOR', (1152,), 'int32'), ('TENSOR', (5,), 'int32')). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCHW passed\n"
     ]
    }
   ],
   "source": [
    "# NCHW\n",
    "\n",
    "data = np.random.rand(N, C, HW, HW).astype('float32')\n",
    "weight = spweight.toarray().reshape(C, C, 3, 3)\n",
    "Data = te.placeholder(data.shape, 'float32')\n",
    "\n",
    "## dense\n",
    "\n",
    "Weight = te.placeholder(weight.shape, 'float32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.nn.conv2d_nchw(Data, Weight, 1, 1, 1, 'float32')\n",
    "    s = topi.generic.schedule_conv2d_nchw(CC)\n",
    "    func = tvm.build(s, [Data, Weight, CC])\n",
    "args = [nd.array(data), nd.array(weight), nd.empty(CC.shape)]\n",
    "func(*args)\n",
    "\n",
    "## sparse\n",
    "\n",
    "Wdat = te.placeholder(spweight.data.shape, 'float32')\n",
    "Wind = te.placeholder(spweight.indices.shape, 'int32')\n",
    "Wptr = te.placeholder(spweight.indptr.shape, 'int32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.x86.sparse.spconv2d_3x3_nchw(Data, Wdat, Wind, Wptr)\n",
    "    s = topi.x86.sparse.schedule_spconv2d_3x3_nchw(CC)\n",
    "    func = tvm.build(s, [Data, Wdat, Wind, Wptr, CC])\n",
    "args2 = [nd.array(data), nd.array(spweight.data), nd.array(spweight.indices), nd.array(spweight.indptr), nd.empty(CC.shape)]\n",
    "func(*args2)\n",
    "\n",
    "## assert\n",
    "\n",
    "testing.assert_allclose(args[-1].numpy(), args2[-1].numpy())\n",
    "print('NCHW passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline model\n",
    "\n",
    "1. load the model from onnx, and convert it to tvm.relay (with args inlined as const nodes in the graph)\n",
    "2. extract tunable autotvm tasks from the relay model\n",
    "3. tune the tasks, results saved in `test_dense.best.log` (text file)\n",
    "4. build the model and execute with `debug_executor` to see the time by each operator\n",
    "\n",
    "`OMP_NUM_THREADS` controls the thread numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS 1\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"sparse_resnet18_best_onnx/resnet18_GL_16_PR_0.6_ckpt_best.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import relay\n",
    "const_mod, params = relay.frontend.from_onnx(onnx_model, {'data': (10, 3, 224, 224)}, freeze_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) {',\n",
       " '  %0 = nn.conv2d(%data, meta[relay.Constant][0], strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7]);',\n",
       " '  %1 = nn.bias_add(%0, meta[relay.Constant][1]);',\n",
       " '  %2 = nn.relu(%1);',\n",
       " '  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);',\n",
       " '  %4 = nn.conv2d(%3, meta[relay.Constant][2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %5 = nn.bias_add(%4, meta[relay.Constant][3]);',\n",
       " '  %6 = nn.relu(%5);',\n",
       " '  %7 = nn.conv2d(%6, meta[relay.Constant][4], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %8 = nn.bias_add(%7, meta[relay.Constant][5]);',\n",
       " '  %9 = add(%8, %3);',\n",
       " '  %10 = nn.relu(%9);',\n",
       " '  %11 = nn.conv2d(%10, meta[relay.Constant][6], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %12 = nn.bias_add(%11, meta[relay.Constant][7]);',\n",
       " '  %13 = nn.relu(%12);',\n",
       " '  %14 = nn.conv2d(%13, meta[relay.Constant][8], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %15 = nn.bias_add(%14, meta[relay.Constant][9]);',\n",
       " '  %16 = add(%15, %10);',\n",
       " '  %17 = nn.relu(%16);',\n",
       " '  %18 = nn.conv2d(%17, meta[relay.Constant][10], strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %19 = nn.bias_add(%18, meta[relay.Constant][11]);',\n",
       " '  %20 = nn.relu(%19);',\n",
       " '  %21 = nn.conv2d(%20, meta[relay.Constant][12], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %22 = nn.conv2d(%17, meta[relay.Constant][14], strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]);',\n",
       " '  %23 = nn.bias_add(%21, meta[relay.Constant][13]);',\n",
       " '  %24 = nn.bias_add(%22, meta[relay.Constant][15]);',\n",
       " '  %25 = add(%23, %24);',\n",
       " '  %26 = nn.relu(%25);',\n",
       " '  %27 = nn.conv2d(%26, meta[relay.Constant][16], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %28 = nn.bias_add(%27, meta[relay.Constant][17]);',\n",
       " '  %29 = nn.relu(%28);',\n",
       " '  %30 = nn.conv2d(%29, meta[relay.Constant][18], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %31 = nn.bias_add(%30, meta[relay.Constant][19]);',\n",
       " '  %32 = add(%31, %26);',\n",
       " '  %33 = nn.relu(%32);',\n",
       " '  %34 = nn.conv2d(%33, meta[relay.Constant][20], strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %35 = nn.bias_add(%34, meta[relay.Constant][21]);',\n",
       " '  %36 = nn.relu(%35);',\n",
       " '  %37 = nn.conv2d(%36, meta[relay.Constant][22], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %38 = nn.conv2d(%33, meta[relay.Constant][24], strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]);',\n",
       " '  %39 = nn.bias_add(%37, meta[relay.Constant][23]);',\n",
       " '  %40 = nn.bias_add(%38, meta[relay.Constant][25]);',\n",
       " '  %41 = add(%39, %40);',\n",
       " '  %42 = nn.relu(%41);',\n",
       " '  %43 = nn.conv2d(%42, meta[relay.Constant][26], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %44 = nn.bias_add(%43, meta[relay.Constant][27]);',\n",
       " '  %45 = nn.relu(%44);',\n",
       " '  %46 = nn.conv2d(%45, meta[relay.Constant][28], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %47 = nn.bias_add(%46, meta[relay.Constant][29]);',\n",
       " '  %48 = add(%47, %42);',\n",
       " '  %49 = nn.relu(%48);',\n",
       " '  %50 = nn.conv2d(%49, meta[relay.Constant][30], strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %51 = nn.bias_add(%50, meta[relay.Constant][31]);',\n",
       " '  %52 = nn.relu(%51);',\n",
       " '  %53 = nn.conv2d(%52, meta[relay.Constant][32], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %54 = nn.conv2d(%49, meta[relay.Constant][34], strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]);',\n",
       " '  %55 = nn.bias_add(%53, meta[relay.Constant][33]);',\n",
       " '  %56 = nn.bias_add(%54, meta[relay.Constant][35]);',\n",
       " '  %57 = add(%55, %56);',\n",
       " '  %58 = nn.relu(%57);',\n",
       " '  %59 = nn.conv2d(%58, meta[relay.Constant][36], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %60 = nn.bias_add(%59, meta[relay.Constant][37]);',\n",
       " '  %61 = nn.relu(%60);',\n",
       " '  %62 = nn.conv2d(%61, meta[relay.Constant][38], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %63 = nn.bias_add(%62, meta[relay.Constant][39]);',\n",
       " '  %64 = add(%63, %58);',\n",
       " '  %65 = nn.relu(%64);',\n",
       " '  %66 = nn.global_avg_pool2d(%65);',\n",
       " '  %67 = reshape(%66, newshape=[10, -1]);',\n",
       " '  %68 = nn.batch_flatten(%67);',\n",
       " '  %69 = nn.dense(%68, meta[relay.Constant][40], units=1000);',\n",
       " '  add(%69, meta[relay.Constant][41])',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ',\n",
       " '      \"data\": [2]',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Array\", ',\n",
       " '      \"data\": [',\n",
       " '        3, ',\n",
       " '        4, ',\n",
       " '        5, ',\n",
       " '        6, ',\n",
       " '        7, ',\n",
       " '        8, ',\n",
       " '        9, ',\n",
       " '        10, ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_mod.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import autotvm\n",
    "\n",
    "tasks = autotvm.task.extract_from_program(\n",
    "    const_mod['main'], params={}, target='llvm -mcpu=cascadelake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:  115.19/ 197.50 GFLOPS | Progress: (200/200) | 448.85 s Done.\n",
      "conv2d_NCHWc.x86 980\n",
      " Current/Best:    9.94/ 203.54 GFLOPS | Progress: (200/200) | 390.84 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:    7.72/ 183.42 GFLOPS | Progress: (200/200) | 328.03 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   68.05/ 143.04 GFLOPS | Progress: (200/200) | 290.39 s Done.\n",
      "conv2d_NCHWc.x86 1024\n",
      " Current/Best:   30.71/ 212.58 GFLOPS | Progress: (200/200) | 397.88 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   40.97/ 178.58 GFLOPS | Progress: (200/200) | 312.93 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   43.79/ 166.37 GFLOPS | Progress: (200/200) | 286.90 s Done.\n",
      "conv2d_NCHWc.x86 972\n",
      " Current/Best:  144.97/ 211.71 GFLOPS | Progress: (200/200) | 443.07 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   64.33/ 159.66 GFLOPS | Progress: (200/200) | 367.98 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   89.31/ 181.01 GFLOPS | Progress: (200/200) | 278.24 s Done.\n",
      "conv2d_NCHWc.x86 800\n",
      " Current/Best:   97.93/ 178.50 GFLOPS | Progress: (200/200) | 497.59 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   41.85/  63.25 GFLOPS | Progress: (200/200) | 299.72 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:    7.83/  91.29 GFLOPS | Progress: (200/200) | 334.28 s Done.\n"
     ]
    }
   ],
   "source": [
    "opts = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(timeout=20, min_repeat_ms=200),\n",
    ")\n",
    "\n",
    "for tsk in tasks:\n",
    "    print(tsk.name, len(tsk.config_space))\n",
    "    nsamples = min(200, len(tsk.config_space))\n",
    "    tuner = autotvm.tuner.GATuner(tsk)\n",
    "    tuner.tune(\n",
    "        nsamples,\n",
    "        measure_option=opts,\n",
    "        callbacks=[\n",
    "            autotvm.callback.progress_bar(nsamples),\n",
    "            autotvm.callback.log_to_file('test_dense.log'),\n",
    "        ],\n",
    "    )\n",
    "autotvm.record.pick_best('test_dense.log', 'test_dense.best.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Name                                                        Ops                                                             Time(us)    Time(%)  Shape                  Inputs  Outputs  \n",
      "---------                                                        ---                                                             --------    -------  -----                  ------  -------  \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu         tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu        16015.9     7.553    (10, 2, 112, 112, 32)  3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_31  tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3  15272.8     7.203    (10, 16, 7, 7, 32)     4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7      15067.0     7.106    (10, 16, 7, 7, 32)     3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3   tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3  14927.2     7.04     (10, 16, 7, 7, 32)     4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu     tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu    12448.3     5.871    (10, 1, 56, 56, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      12292.0     5.797    (10, 1, 56, 56, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu1    tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu    12226.6     5.766    (10, 1, 56, 56, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      11921.9     5.622    (10, 1, 56, 56, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1   tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1  11597.0     5.469    (10, 2, 28, 28, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2   tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2  11557.1     5.45     (10, 8, 14, 14, 32)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_21  tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2  11519.3     5.432    (10, 8, 14, 14, 32)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5      11467.9     5.408    (10, 8, 14, 14, 32)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_11  tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1  11465.0     5.407    (10, 2, 28, 28, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3      11273.8     5.317    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6      7119.42     3.357    (10, 16, 7, 7, 32)     3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2      6949.68     3.277    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4      6840.68     3.226    (10, 4, 14, 14, 64)    3       1        \n",
      "tvmgen_default_fused_nn_max_pool2d                               tvmgen_default_fused_nn_max_pool2d                              3213.29     1.515    (10, 2, 56, 56, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_1                          tvmgen_default_fused_layout_transform_1                         1127.14     0.532    (10, 1, 56, 56, 64)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                 tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                1073.58     0.506    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_layout_transform                            tvmgen_default_fused_layout_transform                           952.402     0.449    (10, 3, 224, 224, 1)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2               tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2              946.026     0.446    (10, 8, 7, 7, 64)      3       1        \n",
      "tvmgen_default_fused_layout_transform_2                          tvmgen_default_fused_layout_transform_2                         867.997     0.409    (10, 2, 56, 56, 32)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1               tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1              830.202     0.392    (10, 8, 14, 14, 32)    3       1        \n",
      "tvmgen_default_fused_layout_transform_3                          tvmgen_default_fused_layout_transform_3                         411.533     0.194    (10, 8, 28, 28, 16)    1       1        \n",
      "tvmgen_default_fused_layout_transform_31                         tvmgen_default_fused_layout_transform_3                         359.782     0.17     (10, 8, 28, 28, 16)    1       1        \n",
      "tvmgen_default_fused_layout_transform_32                         tvmgen_default_fused_layout_transform_3                         335.402     0.158    (10, 8, 28, 28, 16)    1       1        \n",
      "tvmgen_default_fused_layout_transform_4                          tvmgen_default_fused_layout_transform_4                         303.86      0.143    (10, 4, 28, 28, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_5                          tvmgen_default_fused_layout_transform_5                         302.23      0.143    (10, 1, 28, 28, 128)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_dense_pack_add                   tvmgen_default_fused_nn_contrib_dense_pack_add                  230.737     0.109    (10, 1000)             3       1        \n",
      "tvmgen_default_fused_layout_transform_9                          tvmgen_default_fused_layout_transform_9                         162.804     0.077    (10, 2, 14, 14, 128)   1       1        \n",
      "tvmgen_default_fused_layout_transform_61                         tvmgen_default_fused_layout_transform_6                         156.1       0.074    (10, 4, 14, 14, 64)    1       1        \n",
      "tvmgen_default_fused_layout_transform_6                          tvmgen_default_fused_layout_transform_6                         154.965     0.073    (10, 4, 14, 14, 64)    1       1        \n",
      "tvmgen_default_fused_layout_transform_7                          tvmgen_default_fused_layout_transform_7                         154.649     0.073    (10, 1, 14, 14, 256)   1       1        \n",
      "tvmgen_default_fused_layout_transform_10                         tvmgen_default_fused_layout_transform_10                        113.334     0.053    (10, 16, 7, 7, 32)     1       1        \n",
      "tvmgen_default_fused_nn_global_avg_pool2d                        tvmgen_default_fused_nn_global_avg_pool2d                       103.965     0.049    (10, 16, 1, 1, 32)     1       1        \n",
      "tvmgen_default_fused_layout_transform_81                         tvmgen_default_fused_layout_transform_8                         101.994     0.048    (10, 32, 7, 7, 16)     1       1        \n",
      "tvmgen_default_fused_layout_transform_82                         tvmgen_default_fused_layout_transform_8                         94.371      0.045    (10, 32, 7, 7, 16)     1       1        \n",
      "tvmgen_default_fused_layout_transform_8                          tvmgen_default_fused_layout_transform_8                         84.365      0.04     (10, 32, 7, 7, 16)     1       1        \n",
      "tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten   tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten  3.399       0.002    (10, 512)              1       1        \n",
      "Total_time                                                       -                                                               212045.707  -        -                      -       -        \n"
     ]
    }
   ],
   "source": [
    "from tvm import autotvm\n",
    "from tvm.contrib.debugger import debug_executor\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "with autotvm.apply_history_best('test_dense.best.log'):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build_module.build(const_mod, params={}, target='llvm -mcpu=cascadelake')\n",
    "\n",
    "dev = tvm.device('llvm -mcpu=cascadelake', 0)\n",
    "data = tvm.nd.array(np.random.rand(10, 3, 224, 224).astype('float32'))\n",
    "\n",
    "graph_dense = debug_executor.create(lib.graph_json, lib.module, dev)\n",
    "graph_dense.set_input(data=data, **lib.params)\n",
    "\n",
    "graph_dense.run()\n",
    "#ftimer = graph_dense.module.time_evaluator(\"run\", dev, number=100, repeat=1)\n",
    "#ftimer().mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the NCHW model to sparse with a C++ pass `Conv2dToSparse2`\n",
    "newfunc = relay.data_dep_optimization.utils._run_opt_pass(\n",
    "    const_mod['main'],\n",
    "    # layout, kernel_size, bsr_height, bsr_width, sparsity_threshold\n",
    "    relay.transform._ffi_api.Conv2dToSparse2(\"NCHW\", 3, 16, 1, 0.4)\n",
    ")\n",
    "spconst_mod = tvm.ir.IRModule.from_expr(newfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) -> Tensor[(10, 1000), float32] {',\n",
       " '  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7]) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %2 = nn.relu(%1) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %4 = nn.sparse_conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(927), int32] */, meta[relay.Constant][4] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %5 = nn.bias_add(%4, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %6 = nn.relu(%5) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %7 = nn.sparse_conv2d(%6, meta[relay.Constant][6] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][7] /* ty=Tensor[(927), int32] */, meta[relay.Constant][8] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %8 = nn.bias_add(%7, meta[relay.Constant][9] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %9 = add(%8, %3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %10 = nn.relu(%9) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %11 = nn.sparse_conv2d(%10, meta[relay.Constant][10] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(927), int32] */, meta[relay.Constant][12] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %12 = nn.bias_add(%11, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %13 = nn.relu(%12) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %14 = nn.sparse_conv2d(%13, meta[relay.Constant][14] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][15] /* ty=Tensor[(927), int32] */, meta[relay.Constant][16] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %15 = nn.bias_add(%14, meta[relay.Constant][17] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %16 = add(%15, %10) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %17 = nn.relu(%16) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %18 = nn.conv2d(%17, meta[relay.Constant][18] /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %19 = nn.bias_add(%18, meta[relay.Constant][19] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %20 = nn.relu(%19) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %21 = nn.sparse_conv2d(%20, meta[relay.Constant][20] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][22] /* ty=Tensor[(9), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %22 = nn.conv2d(%17, meta[relay.Constant][24] /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %23 = nn.bias_add(%21, meta[relay.Constant][23] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %24 = nn.bias_add(%22, meta[relay.Constant][25] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %25 = add(%23, %24) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %26 = nn.relu(%25) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %27 = nn.sparse_conv2d(%26, meta[relay.Constant][26] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][27] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][28] /* ty=Tensor[(9), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %28 = nn.bias_add(%27, meta[relay.Constant][29] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %29 = nn.relu(%28) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %30 = nn.sparse_conv2d(%29, meta[relay.Constant][30] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][32] /* ty=Tensor[(9), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %31 = nn.bias_add(%30, meta[relay.Constant][33] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %32 = add(%31, %26) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %33 = nn.relu(%32) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %34 = nn.conv2d(%33, meta[relay.Constant][34] /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %35 = nn.bias_add(%34, meta[relay.Constant][35] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %36 = nn.relu(%35) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %37 = nn.sparse_conv2d(%36, meta[relay.Constant][36] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][37] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][38] /* ty=Tensor[(17), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %38 = nn.conv2d(%33, meta[relay.Constant][40] /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %39 = nn.bias_add(%37, meta[relay.Constant][39] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %40 = nn.bias_add(%38, meta[relay.Constant][41] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %41 = add(%39, %40) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %42 = nn.relu(%41) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %43 = nn.sparse_conv2d(%42, meta[relay.Constant][42] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][43] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][44] /* ty=Tensor[(17), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %44 = nn.bias_add(%43, meta[relay.Constant][45] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %45 = nn.relu(%44) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %46 = nn.sparse_conv2d(%45, meta[relay.Constant][46] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][47] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][48] /* ty=Tensor[(17), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %47 = nn.bias_add(%46, meta[relay.Constant][49] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %48 = add(%47, %42) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %49 = nn.relu(%48) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %50 = nn.conv2d(%49, meta[relay.Constant][50] /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %51 = nn.bias_add(%50, meta[relay.Constant][51] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %52 = nn.relu(%51) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %53 = nn.sparse_conv2d(%52, meta[relay.Constant][52] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][53] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][54] /* ty=Tensor[(33), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %54 = nn.conv2d(%49, meta[relay.Constant][56] /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %55 = nn.bias_add(%53, meta[relay.Constant][55] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %56 = nn.bias_add(%54, meta[relay.Constant][57] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %57 = add(%55, %56) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %58 = nn.relu(%57) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %59 = nn.sparse_conv2d(%58, meta[relay.Constant][58] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][59] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][60] /* ty=Tensor[(33), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %60 = nn.bias_add(%59, meta[relay.Constant][61] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %61 = nn.relu(%60) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %62 = nn.sparse_conv2d(%61, meta[relay.Constant][62] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][63] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][64] /* ty=Tensor[(33), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %63 = nn.bias_add(%62, meta[relay.Constant][65] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %64 = add(%63, %58) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %65 = nn.relu(%64) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %66 = nn.global_avg_pool2d(%65) /* ty=Tensor[(10, 512, 1, 1), float32] */;',\n",
       " '  %67 = reshape(%66, newshape=[10, -1]) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %68 = nn.batch_flatten(%67) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %69 = nn.dense(%68, meta[relay.Constant][66] /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(10, 1000), float32] */;',\n",
       " '  add(%69, meta[relay.Constant][67] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(10, 1000), float32] */',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ',\n",
       " '      \"data\": [2]',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Array\", ',\n",
       " '      \"data\": [',\n",
       " '        3, ',\n",
       " '        10, ',\n",
       " '        13, ',\n",
       " '        19, ',\n",
       " '        23, ',\n",
       " '        27, ',\n",
       " '        31, ',\n",
       " '        37, ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spconst_mod.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the const model to NHWC, which is better for the sparse implementation\n",
    "desired_layouts = {'nn.conv2d': ['NHWC', 'default']}\n",
    "seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                relay.transform.ConvertLayout(desired_layouts),\n",
    "                                relay.transform.FoldConstant()])\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    const_mod2 = seq(const_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, convert to sparse, but NHWC this time\n",
    "# bsr block size same as NCHW; blocking is done on transposed weight;\n",
    "newfunc = relay.data_dep_optimization.utils._run_opt_pass(\n",
    "    const_mod2['main'],\n",
    "    relay.transform._ffi_api.Conv2dToSparse2(\"NHWC\", 3, 16, 1, 0.4)\n",
    ")\n",
    "spconst_mod2 = tvm.ir.IRModule.from_expr(newfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) -> Tensor[(10, 1000), float32] {',\n",
       " '  %0 = layout_transform(%data, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 224, 224, 3), float32] */;',\n",
       " '  %1 = nn.conv2d(%0, meta[relay.Constant][0] /* ty=Tensor[(7, 7, 3, 64), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 112, 112, 64), float32] */;',\n",
       " '  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 112, 112, 64), float32] */;',\n",
       " '  %3 = nn.relu(%2) /* ty=Tensor[(10, 112, 112, 64), float32] */;',\n",
       " '  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1], layout=\"NHWC\") /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %5 = nn.sparse_conv2d(%4, meta[relay.Constant][2] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(927), int32] */, meta[relay.Constant][4] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %6 = add(%5, meta[relay.Constant][5] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %7 = nn.relu(%6) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %8 = nn.sparse_conv2d(%7, meta[relay.Constant][6] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][7] /* ty=Tensor[(927), int32] */, meta[relay.Constant][8] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %9 = add(%8, meta[relay.Constant][9] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %10 = add(%9, %4) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %11 = nn.relu(%10) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %12 = nn.sparse_conv2d(%11, meta[relay.Constant][10] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(927), int32] */, meta[relay.Constant][12] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %13 = add(%12, meta[relay.Constant][13] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %14 = nn.relu(%13) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %15 = nn.sparse_conv2d(%14, meta[relay.Constant][14] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][15] /* ty=Tensor[(927), int32] */, meta[relay.Constant][16] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %16 = add(%15, meta[relay.Constant][17] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %17 = add(%16, %11) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %18 = nn.relu(%17) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %19 = nn.conv2d(%18, meta[relay.Constant][18] /* ty=Tensor[(3, 3, 64, 128), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %20 = add(%19, meta[relay.Constant][19] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %21 = nn.relu(%20) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %22 = nn.sparse_conv2d(%21, meta[relay.Constant][20] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][22] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %23 = nn.conv2d(%18, meta[relay.Constant][24] /* ty=Tensor[(1, 1, 64, 128), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %24 = add(%22, meta[relay.Constant][23] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %25 = add(%23, meta[relay.Constant][25] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %26 = add(%24, %25) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %27 = nn.relu(%26) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %28 = nn.sparse_conv2d(%27, meta[relay.Constant][26] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][27] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][28] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %29 = add(%28, meta[relay.Constant][29] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %30 = nn.relu(%29) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %31 = nn.sparse_conv2d(%30, meta[relay.Constant][30] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][32] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %32 = add(%31, meta[relay.Constant][33] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %33 = add(%32, %27) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %34 = nn.relu(%33) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %35 = nn.conv2d(%34, meta[relay.Constant][34] /* ty=Tensor[(3, 3, 128, 256), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %36 = add(%35, meta[relay.Constant][35] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %37 = nn.relu(%36) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %38 = nn.sparse_conv2d(%37, meta[relay.Constant][36] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][37] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][38] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %39 = nn.conv2d(%34, meta[relay.Constant][40] /* ty=Tensor[(1, 1, 128, 256), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %40 = add(%38, meta[relay.Constant][39] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %41 = add(%39, meta[relay.Constant][41] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %42 = add(%40, %41) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %43 = nn.relu(%42) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %44 = nn.sparse_conv2d(%43, meta[relay.Constant][42] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][43] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][44] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %45 = add(%44, meta[relay.Constant][45] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %46 = nn.relu(%45) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %47 = nn.sparse_conv2d(%46, meta[relay.Constant][46] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][47] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][48] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %48 = add(%47, meta[relay.Constant][49] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %49 = add(%48, %43) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %50 = nn.relu(%49) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %51 = nn.conv2d(%50, meta[relay.Constant][50] /* ty=Tensor[(3, 3, 256, 512), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %52 = add(%51, meta[relay.Constant][51] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %53 = nn.relu(%52) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %54 = nn.sparse_conv2d(%53, meta[relay.Constant][52] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][53] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][54] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %55 = nn.conv2d(%50, meta[relay.Constant][56] /* ty=Tensor[(1, 1, 256, 512), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %56 = add(%54, meta[relay.Constant][55] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %57 = add(%55, meta[relay.Constant][57] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %58 = add(%56, %57) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %59 = nn.relu(%58) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %60 = nn.sparse_conv2d(%59, meta[relay.Constant][58] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][59] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][60] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %61 = add(%60, meta[relay.Constant][61] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %62 = nn.relu(%61) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %63 = nn.sparse_conv2d(%62, meta[relay.Constant][62] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][63] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][64] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %64 = add(%63, meta[relay.Constant][65] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %65 = add(%64, %59) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %66 = nn.relu(%65) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %67 = nn.global_avg_pool2d(%66, layout=\"NHWC\") /* ty=Tensor[(10, 1, 1, 512), float32] */;',\n",
       " '  %68 = layout_transform(%67, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 512, 1, 1), float32] */;',\n",
       " '  %69 = reshape(%68, newshape=[10, -1]) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %70 = nn.batch_flatten(%69) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %71 = nn.dense(%70, meta[relay.Constant][66] /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(10, 1000), float32] */;',\n",
       " '  add(%71, meta[relay.Constant][67] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(10, 1000), float32] */',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ',\n",
       " '      \"data\": [2]',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Array\", ',\n",
       " '      \"data\": [',\n",
       " '        3, ',\n",
       " '        10, ',\n",
       " '        17, ',\n",
       " '        23, ',\n",
       " '        27, ',\n",
       " '        31, ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spconst_mod2.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the NHWC sparse model back to NCHW, to utilize performant dense conv2d_nchwc\n",
    "# sparse conv2d won't be converted back; they are not layout-transformation aware\n",
    "desired_layouts = {'nn.conv2d': ['NCHW', 'default']}\n",
    "seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                relay.transform.ConvertLayout(desired_layouts),\n",
    "                                relay.transform.FoldConstant()])\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    spconst_mod3 = seq(spconst_mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) -> Tensor[(10, 1000), float32] {',\n",
       " '  %0 = layout_transform(%data, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 224, 224, 3), float32] */;',\n",
       " '  %1 = layout_transform(%0, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 3, 224, 224), float32] */;',\n",
       " '  %2 = nn.conv2d(%1, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7]) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %3 = add(%2, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 1, 1), float32] */) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %4 = nn.relu(%3) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %5 = nn.max_pool2d(%4, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %6 = layout_transform(%5, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %7 = nn.sparse_conv2d(%6, meta[relay.Constant][2] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(927), int32] */, meta[relay.Constant][4] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %8 = add(%7, meta[relay.Constant][5] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %9 = nn.relu(%8) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %10 = nn.sparse_conv2d(%9, meta[relay.Constant][6] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][7] /* ty=Tensor[(927), int32] */, meta[relay.Constant][8] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %11 = add(%10, meta[relay.Constant][9] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %12 = add(%11, %6) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %13 = nn.relu(%12) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %14 = nn.sparse_conv2d(%13, meta[relay.Constant][10] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(927), int32] */, meta[relay.Constant][12] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %15 = add(%14, meta[relay.Constant][13] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %16 = nn.relu(%15) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %17 = nn.sparse_conv2d(%16, meta[relay.Constant][14] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][15] /* ty=Tensor[(927), int32] */, meta[relay.Constant][16] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %18 = add(%17, meta[relay.Constant][17] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %19 = add(%18, %13) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %20 = nn.relu(%19) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %21 = layout_transform(%20, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %22 = nn.conv2d(%21, meta[relay.Constant][18] /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %23 = add(%22, meta[relay.Constant][19] /* ty=Tensor[(1, 128, 1, 1), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %24 = nn.relu(%23) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %25 = layout_transform(%24, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %26 = nn.sparse_conv2d(%25, meta[relay.Constant][20] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][22] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %27 = nn.conv2d(%21, meta[relay.Constant][24] /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %28 = add(%27, meta[relay.Constant][25] /* ty=Tensor[(1, 128, 1, 1), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %29 = add(%26, meta[relay.Constant][23] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %30 = layout_transform(%28, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %31 = add(%29, %30) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %32 = nn.relu(%31) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %33 = nn.sparse_conv2d(%32, meta[relay.Constant][26] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][27] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][28] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %34 = add(%33, meta[relay.Constant][29] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %35 = nn.relu(%34) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %36 = nn.sparse_conv2d(%35, meta[relay.Constant][30] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][32] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %37 = add(%36, meta[relay.Constant][33] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %38 = add(%37, %32) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %39 = nn.relu(%38) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %40 = layout_transform(%39, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %41 = nn.conv2d(%40, meta[relay.Constant][34] /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %42 = add(%41, meta[relay.Constant][35] /* ty=Tensor[(1, 256, 1, 1), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %43 = nn.relu(%42) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %44 = layout_transform(%43, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %45 = nn.sparse_conv2d(%44, meta[relay.Constant][36] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][37] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][38] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %46 = nn.conv2d(%40, meta[relay.Constant][40] /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %47 = add(%46, meta[relay.Constant][41] /* ty=Tensor[(1, 256, 1, 1), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %48 = add(%45, meta[relay.Constant][39] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %49 = layout_transform(%47, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %50 = add(%48, %49) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %51 = nn.relu(%50) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %52 = nn.sparse_conv2d(%51, meta[relay.Constant][42] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][43] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][44] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %53 = add(%52, meta[relay.Constant][45] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %54 = nn.relu(%53) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %55 = nn.sparse_conv2d(%54, meta[relay.Constant][46] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][47] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][48] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %56 = add(%55, meta[relay.Constant][49] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %57 = add(%56, %51) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %58 = nn.relu(%57) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %59 = layout_transform(%58, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %60 = nn.conv2d(%59, meta[relay.Constant][50] /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %61 = add(%60, meta[relay.Constant][51] /* ty=Tensor[(1, 512, 1, 1), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %62 = nn.relu(%61) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %63 = layout_transform(%62, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %64 = nn.sparse_conv2d(%63, meta[relay.Constant][52] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][53] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][54] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %65 = nn.conv2d(%59, meta[relay.Constant][56] /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %66 = add(%65, meta[relay.Constant][57] /* ty=Tensor[(1, 512, 1, 1), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %67 = add(%64, meta[relay.Constant][55] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %68 = layout_transform(%66, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %69 = add(%67, %68) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %70 = nn.relu(%69) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %71 = nn.sparse_conv2d(%70, meta[relay.Constant][58] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][59] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][60] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %72 = add(%71, meta[relay.Constant][61] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %73 = nn.relu(%72) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %74 = nn.sparse_conv2d(%73, meta[relay.Constant][62] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][63] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][64] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %75 = add(%74, meta[relay.Constant][65] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %76 = add(%75, %70) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %77 = nn.relu(%76) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %78 = nn.global_avg_pool2d(%77, layout=\"NHWC\") /* ty=Tensor[(10, 1, 1, 512), float32] */;',\n",
       " '  %79 = layout_transform(%78, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 512, 1, 1), float32] */;',\n",
       " '  %80 = reshape(%79, newshape=[10, -1]) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %81 = nn.batch_flatten(%80) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %82 = nn.dense(%81, meta[relay.Constant][66] /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(10, 1000), float32] */;',\n",
       " '  add(%82, meta[relay.Constant][67] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(10, 1000), float32] */',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spconst_mod3.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse tuning\n",
    "\n",
    "differences from dense tuning:\n",
    "\n",
    "1. sparse tuning needs data feeding\n",
    "2. so we extract the data first (in `spconv_data`)\n",
    "3. then feed the data to `.ref_input` during tuning\n",
    "\n",
    "results in `test_sparse.best.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import autotvm\n",
    "\n",
    "tasks = autotvm.task.extract_from_program(\n",
    "    spconst_mod['main'], params={}, target='llvm -mcpu=cascadelake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "spconv_data = {}\n",
    "\n",
    "def fvisit(e):\n",
    "    if isinstance(e, relay.Call) and e.op.name == 'nn.sparse_conv2d':\n",
    "        args_type = [i.checked_type for i in e.args]\n",
    "        args_type = tuple(('TENSOR', i.concrete_shape, i.dtype) for i in args_type)\n",
    "        weight = tuple(i.data.numpy() for i in e.args[1:])\n",
    "        weight = sparse.bsr_matrix(weight)\n",
    "        spconv_data.setdefault(args_type, []).append(weight)\n",
    "\n",
    "# data extraction\n",
    "relay.analysis.post_order_visit(spconst_mod['main'], fvisit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:    0.00/   0.00 GFLOPS | Progress: (0/308) | 0.00 s"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "opts = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(timeout=20, min_repeat_ms=200),\n",
    ")\n",
    "\n",
    "for tsk in tasks:\n",
    "    opts['runner'].ref_input = None\n",
    "    if tsk.name.startswith('conv3x3_sp'):\n",
    "        # feed the sparse data\n",
    "        data, wdat, wind, wptr, *attrs = tsk.args\n",
    "        weight = random.choice(spconv_data[data, wdat, wind, wptr])\n",
    "        wdat, wind, wptr = weight.data, weight.indices, weight.indptr\n",
    "        data = np.random.rand(*data[1]).astype(data[2])\n",
    "        ret = np.zeros_like(data)\n",
    "        # arg ordering is reversed\n",
    "        # arg ordering of tasks extracted does not respect the relay op definition\n",
    "        # it is generated by pre-order(?) visit\n",
    "        opts['runner'].ref_input = [ret, wptr, wind, wdat, data]\n",
    "    print(tsk.name, len(tsk.config_space))\n",
    "    nsamples = min(1000, len(tsk.config_space))\n",
    "    tuner = autotvm.tuner.GATuner(tsk)\n",
    "    tuner.tune(\n",
    "        nsamples,\n",
    "        measure_option=opts,\n",
    "        callbacks=[\n",
    "            autotvm.callback.progress_bar(nsamples),\n",
    "            autotvm.callback.log_to_file('test_sparse.log'),\n",
    "        ],\n",
    "    )\n",
    "autotvm.record.pick_best('test_sparse.log', 'test_sparse.best.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Name                                                       Ops                                                             Time(us)    Time(%)  Shape                  Inputs  Outputs  \n",
      "---------                                                       ---                                                             --------    -------  -----                  ------  -------  \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu        tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu        15464.8     8.613    (10, 2, 112, 112, 32)  3       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu           tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu           10226.9     5.696    (10, 56, 56, 64)       6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu1          tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu           10086.6     5.618    (10, 56, 56, 64)       6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_1         tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_1         9962.85     5.549    (10, 28, 28, 128)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu               tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu               9939.41     5.536    (10, 56, 56, 64)       5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_11        tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_1         9936.28     5.534    (10, 28, 28, 128)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_2         tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_2         9900.94     5.515    (10, 14, 14, 256)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_21        tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_2         9890.77     5.509    (10, 14, 14, 256)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_2             tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_2             9771.03     5.442    (10, 14, 14, 256)      5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu1              tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu               9753.11     5.432    (10, 56, 56, 64)       5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_1             tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_1             9697.75     5.401    (10, 28, 28, 128)      5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_3             tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_3             9366.94     5.217    (10, 7, 7, 512)        5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_3         tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_3         9338.83     5.201    (10, 7, 7, 512)        6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_31        tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_3         9289.67     5.174    (10, 7, 7, 512)        6       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      7358.91     4.099    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3      7327.92     4.081    (10, 16, 7, 7, 32)     3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2      6802.51     3.789    (10, 4, 14, 14, 64)    3       1        \n",
      "tvmgen_default_fused_nn_max_pool2d                              tvmgen_default_fused_nn_max_pool2d                              2739.89     1.526    (10, 2, 56, 56, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_1                         tvmgen_default_fused_layout_transform_1                         2142.23     1.193    (10, 64, 56, 56)       1       1        \n",
      "tvmgen_default_fused_layout_transform_3                         tvmgen_default_fused_layout_transform_3                         1305.96     0.727    (10, 1, 56, 56, 64)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                1025.2      0.571    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform          tvmgen_default_fused_layout_transform_layout_transform          896.828     0.5      (10, 56, 56, 64)       1       1        \n",
      "tvmgen_default_fused_layout_transform_4                         tvmgen_default_fused_layout_transform_4                         866.009     0.482    (10, 128, 28, 28)      1       1        \n",
      "tvmgen_default_fused_layout_transform_2                         tvmgen_default_fused_layout_transform_2                         852.749     0.475    (10, 16, 56, 56, 4)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2              tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2              830.98      0.463    (10, 8, 7, 7, 64)      3       1        \n",
      "tvmgen_default_fused_layout_transform                           tvmgen_default_fused_layout_transform                           764.849     0.426    (10, 3, 224, 224, 1)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1              tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1              742.399     0.413    (10, 4, 14, 14, 64)    3       1        \n",
      "tvmgen_default_fused_layout_transform_6                         tvmgen_default_fused_layout_transform_6                         496.324     0.276    (10, 1, 28, 28, 128)   1       1        \n",
      "tvmgen_default_fused_layout_transform_5                         tvmgen_default_fused_layout_transform_5                         424.856     0.237    (10, 4, 28, 28, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_1        tvmgen_default_fused_layout_transform_layout_transform_1        407.473     0.227    (10, 28, 28, 128)      1       1        \n",
      "tvmgen_default_fused_layout_transform_7                         tvmgen_default_fused_layout_transform_7                         391.82      0.218    (10, 256, 14, 14)      1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_11       tvmgen_default_fused_layout_transform_layout_transform_1        309.094     0.172    (10, 28, 28, 128)      1       1        \n",
      "tvmgen_default_fused_layout_transform_9                         tvmgen_default_fused_layout_transform_9                         230.084     0.128    (10, 2, 14, 14, 128)   1       1        \n",
      "tvmgen_default_fused_layout_transform_8                         tvmgen_default_fused_layout_transform_8                         225.525     0.126    (10, 1, 14, 14, 256)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_dense_pack_add                  tvmgen_default_fused_nn_contrib_dense_pack_add                  196.533     0.109    (10, 1000)             3       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_2        tvmgen_default_fused_layout_transform_layout_transform_2        157.076     0.087    (10, 14, 14, 256)      1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_21       tvmgen_default_fused_layout_transform_layout_transform_2        149.554     0.083    (10, 14, 14, 256)      1       1        \n",
      "tvmgen_default_fused_nn_global_avg_pool2d                       tvmgen_default_fused_nn_global_avg_pool2d                       102.294     0.057    (10, 1, 1, 512)        1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_3        tvmgen_default_fused_layout_transform_layout_transform_3        93.378      0.052    (10, 7, 7, 512)        1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_4        tvmgen_default_fused_layout_transform_layout_transform_4        71.902      0.04     (10, 7, 7, 512)        1       1        \n",
      "tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten  tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten  3.419       0.002    (10, 512)              1       1        \n",
      "Total_time                                                      -                                                               179541.646  -        -                      -       -        \n"
     ]
    }
   ],
   "source": [
    "from tvm import autotvm\n",
    "from tvm.contrib.debugger import debug_executor\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "with autotvm.apply_history_best('test_sparse.best.log'):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build_module.build(spconst_mod3, params={}, target='llvm -mcpu=cascadelake')\n",
    "\n",
    "dev = tvm.device('llvm -mcpu=cascadelake', 0)\n",
    "data = tvm.nd.array(np.random.rand(10, 3, 224, 224).astype('float32'))\n",
    "\n",
    "graph_sparse = debug_executor.create(lib.graph_json, lib.module, dev)\n",
    "graph_sparse.set_input(data=data, **lib.params)\n",
    "\n",
    "graph_sparse.run()\n",
    "#ftimer = graph_sparse.module.time_evaluator(\"run\", dev, number=100, repeat=1)\n",
    "#ftimer().mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different sparsity\n",
    "\n",
    "now you've got all code you need. to tuning models at different sparsities, use the following.\n",
    "\n",
    "just modify `Path.glob` and it should work.\n",
    "\n",
    "fast tuning, 200 samples for each op. can be changed\n",
    "\n",
    "slow infer, 100 samples for repeat execution. can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def convert_mod_layout(const_mod, layout):\n",
    "    desired_layouts = {'nn.conv2d': [layout, 'default']}\n",
    "    seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                    relay.transform.ConvertLayout(desired_layouts),\n",
    "                                    relay.transform.FoldConstant()])\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        const_mod2 = seq(const_mod)\n",
    "    return const_mod2\n",
    "\n",
    "def convert_to_sparse(const_mod, layout, kernel, bsrR, bsrC, sprate):\n",
    "    newfunc = relay.data_dep_optimization.utils._run_opt_pass(\n",
    "        const_mod['main'],\n",
    "        relay.transform._ffi_api.Conv2dToSparse2(layout, kernel, bsrR, bsrC, sprate)\n",
    "    )\n",
    "    spconst_mod = tvm.ir.IRModule.from_expr(newfunc)\n",
    "    return spconst_mod\n",
    "    \n",
    "    \n",
    "def extract_spconv2d_data(spconst_mod):\n",
    "    spconv_data = {}\n",
    "\n",
    "    def fvisit(e):\n",
    "        if isinstance(e, relay.Call) and e.op.name == 'nn.sparse_conv2d':\n",
    "            args_type = [i.checked_type for i in e.args]\n",
    "            args_type = tuple(('TENSOR', i.concrete_shape, i.dtype) for i in args_type)\n",
    "            weight = tuple(i.data.numpy() for i in e.args[1:])\n",
    "            weight = sparse.bsr_matrix(weight)\n",
    "            spconv_data.setdefault(args_type, []).append(weight)\n",
    "\n",
    "    relay.analysis.post_order_visit(spconst_mod['main'], fvisit)\n",
    "    return spconv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1620, 16, 1), 'float32'), ('TENSOR', (1620,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1620, 16, 1), 'float32'), ('TENSOR', (1620,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (6453, 16, 1), 'float32'), ('TENSOR', (6453,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (6453, 16, 1), 'float32'), ('TENSOR', (6453,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (25812, 16, 1), 'float32'), ('TENSOR', (25812,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (25812, 16, 1), 'float32'), ('TENSOR', (25812,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (103221, 16, 1), 'float32'), ('TENSOR', (103221,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (103221, 16, 1), 'float32'), ('TENSOR', (103221,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   13.08/ 194.94 GFLOPS | Progress: (200/200) | 537.88 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:    0.00/   0.00 GFLOPS | Progress: (0/200) | 0.00 s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/acct-hpc/hpcjsl/tvm/python/tvm/autotvm/measure/measure_methods.py:258: RuntimeWarning: You are specifying fixed input for tuning the operator. Be sure your input always fits the operator. Some operators may conduct layout transformation during tuning, thus can lead to unexpected behaviors. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current/Best:  112.60/ 131.14 GFLOPS | Progress: (200/200) | 333.16 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   38.77/ 185.19 GFLOPS | Progress: (200/200) | 318.41 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   72.80/ 155.24 GFLOPS | Progress: (200/200) | 289.79 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:  116.78/ 146.95 GFLOPS | Progress: (200/200) | 286.60 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   92.69/ 175.71 GFLOPS | Progress: (200/200) | 343.64 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:    7.84/ 161.60 GFLOPS | Progress: (200/200) | 287.49 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:  121.40/ 146.09 GFLOPS | Progress: (200/200) | 316.88 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:  104.04/ 177.24 GFLOPS | Progress: (200/200) | 361.98 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   31.64/ 169.77 GFLOPS | Progress: (200/200) | 278.96 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:  125.01/ 133.04 GFLOPS | Progress: (200/200) | 313.66 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:    9.90/  61.84 GFLOPS | Progress: (200/200) | 286.04 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:   32.91/  82.13 GFLOPS | Progress: (200/200) | 331.96 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (468, 16, 1), 'float32'), ('TENSOR', (468,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (468, 16, 1), 'float32'), ('TENSOR', (468,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (1845, 16, 1), 'float32'), ('TENSOR', (1845,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (1845, 16, 1), 'float32'), ('TENSOR', (1845,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (7380, 16, 1), 'float32'), ('TENSOR', (7380,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (7380, 16, 1), 'float32'), ('TENSOR', (7380,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (29493, 16, 1), 'float32'), ('TENSOR', (29493,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (29493, 16, 1), 'float32'), ('TENSOR', (29493,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   50.02/ 197.33 GFLOPS | Progress: (200/200) | 435.28 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   52.65/  96.84 GFLOPS | Progress: (200/200) | 358.20 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   45.54/ 170.46 GFLOPS | Progress: (200/200) | 334.88 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   12.18/ 157.81 GFLOPS | Progress: (200/200) | 289.92 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   93.09/  93.98 GFLOPS | Progress: (200/200) | 322.85 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   21.19/ 169.11 GFLOPS | Progress: (200/200) | 290.21 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:   60.89/ 100.59 GFLOPS | Progress: (200/200) | 304.59 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   67.60/ 162.85 GFLOPS | Progress: (200/200) | 332.40 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   17.16/ 175.16 GFLOPS | Progress: (200/200) | 282.62 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   39.34/  92.12 GFLOPS | Progress: (200/200) | 305.67 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   11.45/  61.24 GFLOPS | Progress: (200/200) | 315.98 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:   18.24/  74.20 GFLOPS | Progress: (200/200) | 334.39 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (927, 16, 1), 'float32'), ('TENSOR', (927,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (927, 16, 1), 'float32'), ('TENSOR', (927,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (3690, 16, 1), 'float32'), ('TENSOR', (3690,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (3690, 16, 1), 'float32'), ('TENSOR', (3690,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (14751, 16, 1), 'float32'), ('TENSOR', (14751,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (14751, 16, 1), 'float32'), ('TENSOR', (14751,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (58986, 16, 1), 'float32'), ('TENSOR', (58986,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (58986, 16, 1), 'float32'), ('TENSOR', (58986,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:  135.19/ 197.76 GFLOPS | Progress: (200/200) | 465.24 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   91.19/ 112.90 GFLOPS | Progress: (200/200) | 355.80 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   11.66/ 184.33 GFLOPS | Progress: (200/200) | 308.73 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   98.53/ 144.99 GFLOPS | Progress: (200/200) | 294.95 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   48.60/ 121.72 GFLOPS | Progress: (200/200) | 303.97 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   19.95/ 172.40 GFLOPS | Progress: (200/200) | 309.11 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   45.50/ 183.34 GFLOPS | Progress: (200/200) | 288.01 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:   26.47/ 123.99 GFLOPS | Progress: (200/200) | 324.32 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   62.76/ 161.21 GFLOPS | Progress: (200/200) | 338.17 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:  172.64/ 172.64 GFLOPS | Progress: (200/200) | 275.53 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   85.51/ 118.03 GFLOPS | Progress: (200/200) | 284.32 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   18.96/  59.43 GFLOPS | Progress: (200/200) | 324.78 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:   54.05/  91.42 GFLOPS | Progress: (200/200) | 328.05 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (2079, 16, 1), 'float32'), ('TENSOR', (2079,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (2079, 16, 1), 'float32'), ('TENSOR', (2079,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (8298, 16, 1), 'float32'), ('TENSOR', (8298,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (8298, 16, 1), 'float32'), ('TENSOR', (8298,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (33183, 16, 1), 'float32'), ('TENSOR', (33183,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (33183, 16, 1), 'float32'), ('TENSOR', (33183,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (132714, 16, 1), 'float32'), ('TENSOR', (132714,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (132714, 16, 1), 'float32'), ('TENSOR', (132714,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:  158.97/ 197.88 GFLOPS | Progress: (200/200) | 457.36 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   78.33/ 138.89 GFLOPS | Progress: (200/200) | 339.29 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:  111.66/ 183.21 GFLOPS | Progress: (200/200) | 330.96 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   45.92/ 161.16 GFLOPS | Progress: (200/200) | 298.23 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   29.46/ 152.20 GFLOPS | Progress: (200/200) | 311.14 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   30.31/ 179.17 GFLOPS | Progress: (200/200) | 319.13 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   86.65/ 149.60 GFLOPS | Progress: (200/200) | 296.73 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:  104.25/ 149.88 GFLOPS | Progress: (200/200) | 316.29 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   21.36/ 177.16 GFLOPS | Progress: (200/200) | 344.57 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   77.89/ 182.23 GFLOPS | Progress: (200/200) | 280.15 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   72.30/ 135.12 GFLOPS | Progress: (200/200) | 338.60 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   17.86/  60.39 GFLOPS | Progress: (200/200) | 303.62 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:   45.21/  92.65 GFLOPS | Progress: (200/200) | 343.33 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (234, 16, 1), 'float32'), ('TENSOR', (234,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (234, 16, 1), 'float32'), ('TENSOR', (234,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (927, 16, 1), 'float32'), ('TENSOR', (927,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (927, 16, 1), 'float32'), ('TENSOR', (927,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (3690, 16, 1), 'float32'), ('TENSOR', (3690,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (3690, 16, 1), 'float32'), ('TENSOR', (3690,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (14751, 16, 1), 'float32'), ('TENSOR', (14751,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (14751, 16, 1), 'float32'), ('TENSOR', (14751,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   76.72/ 197.19 GFLOPS | Progress: (200/200) | 430.03 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   28.69/  72.94 GFLOPS | Progress: (200/200) | 383.18 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:  109.92/ 177.04 GFLOPS | Progress: (200/200) | 306.90 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   53.32/ 158.89 GFLOPS | Progress: (200/200) | 298.65 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   32.45/  75.53 GFLOPS | Progress: (200/200) | 329.43 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   57.41/ 178.91 GFLOPS | Progress: (200/200) | 324.22 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   47.94/ 155.72 GFLOPS | Progress: (200/200) | 287.25 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:   41.25/  78.92 GFLOPS | Progress: (200/200) | 332.43 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   37.89/ 177.19 GFLOPS | Progress: (200/200) | 332.77 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   38.91/ 175.17 GFLOPS | Progress: (200/200) | 282.02 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   38.15/  71.91 GFLOPS | Progress: (200/200) | 318.24 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:    4.57/  61.62 GFLOPS | Progress: (200/200) | 299.45 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:   26.48/  74.84 GFLOPS | Progress: (200/200) | 334.91 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (693, 16, 1), 'float32'), ('TENSOR', (693,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (693, 16, 1), 'float32'), ('TENSOR', (693,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (2772, 16, 1), 'float32'), ('TENSOR', (2772,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (2772, 16, 1), 'float32'), ('TENSOR', (2772,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (11061, 16, 1), 'float32'), ('TENSOR', (11061,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (11061, 16, 1), 'float32'), ('TENSOR', (11061,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (44244, 16, 1), 'float32'), ('TENSOR', (44244,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (44244, 16, 1), 'float32'), ('TENSOR', (44244,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   52.42/ 197.75 GFLOPS | Progress: (200/200) | 440.97 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   94.99/ 105.81 GFLOPS | Progress: (200/200) | 365.35 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:  113.13/ 180.94 GFLOPS | Progress: (200/200) | 309.66 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   74.61/ 142.91 GFLOPS | Progress: (200/200) | 298.19 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:  107.01/ 109.58 GFLOPS | Progress: (200/200) | 311.43 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   79.84/ 171.30 GFLOPS | Progress: (200/200) | 319.64 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:  148.82/ 150.50 GFLOPS | Progress: (200/200) | 292.98 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:   94.25/ 114.04 GFLOPS | Progress: (200/200) | 300.60 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:  131.32/ 157.64 GFLOPS | Progress: (200/200) | 360.70 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   82.44/ 176.78 GFLOPS | Progress: (200/200) | 282.47 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   43.11/ 106.92 GFLOPS | Progress: (200/200) | 297.29 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   14.48/  62.34 GFLOPS | Progress: (200/200) | 303.41 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:   67.70/  87.95 GFLOPS | Progress: (200/200) | 342.17 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1152, 16, 1), 'float32'), ('TENSOR', (1152,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1152, 16, 1), 'float32'), ('TENSOR', (1152,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (4608, 16, 1), 'float32'), ('TENSOR', (4608,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (4608, 16, 1), 'float32'), ('TENSOR', (4608,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (18432, 16, 1), 'float32'), ('TENSOR', (18432,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (18432, 16, 1), 'float32'), ('TENSOR', (18432,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (73728, 16, 1), 'float32'), ('TENSOR', (73728,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (73728, 16, 1), 'float32'), ('TENSOR', (73728,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   37.60/ 196.62 GFLOPS | Progress: (200/200) | 450.32 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   99.14/ 118.63 GFLOPS | Progress: (200/200) | 344.24 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   69.93/ 185.56 GFLOPS | Progress: (200/200) | 325.84 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   74.80/ 159.25 GFLOPS | Progress: (200/200) | 299.64 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   99.45/ 136.37 GFLOPS | Progress: (200/200) | 312.36 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   73.01/ 178.93 GFLOPS | Progress: (200/200) | 326.63 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   44.00/ 152.53 GFLOPS | Progress: (200/200) | 286.05 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:   54.22/ 134.66 GFLOPS | Progress: (200/200) | 315.10 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   83.10/ 161.63 GFLOPS | Progress: (200/200) | 331.59 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:  174.99/ 176.02 GFLOPS | Progress: (200/200) | 283.80 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:  115.60/ 126.05 GFLOPS | Progress: (200/200) | 295.96 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   49.30/  55.78 GFLOPS | Progress: (200/200) | 302.95 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:    7.78/  95.61 GFLOPS | Progress: (200/200) | 335.03 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1845, 16, 1), 'float32'), ('TENSOR', (1845,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1845, 16, 1), 'float32'), ('TENSOR', (1845,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (7380, 16, 1), 'float32'), ('TENSOR', (7380,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (7380, 16, 1), 'float32'), ('TENSOR', (7380,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (29493, 16, 1), 'float32'), ('TENSOR', (29493,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (29493, 16, 1), 'float32'), ('TENSOR', (29493,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (117972, 16, 1), 'float32'), ('TENSOR', (117972,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (117972, 16, 1), 'float32'), ('TENSOR', (117972,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   59.36/ 197.84 GFLOPS | Progress: (200/200) | 466.24 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:  121.38/ 136.44 GFLOPS | Progress: (200/200) | 345.84 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:  108.50/ 175.03 GFLOPS | Progress: (200/200) | 331.51 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   74.63/ 138.79 GFLOPS | Progress: (200/200) | 295.92 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   29.75/ 150.59 GFLOPS | Progress: (200/200) | 306.91 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   49.05/ 176.19 GFLOPS | Progress: (200/200) | 330.12 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   46.98/ 154.45 GFLOPS | Progress: (200/200) | 290.17 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:  115.82/ 149.10 GFLOPS | Progress: (200/200) | 315.51 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:  105.00/ 177.13 GFLOPS | Progress: (200/200) | 348.17 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   41.94/ 167.57 GFLOPS | Progress: (200/200) | 282.50 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   11.76/ 133.88 GFLOPS | Progress: (200/200) | 332.88 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:    8.03/  62.32 GFLOPS | Progress: (200/200) | 316.83 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:    9.65/  80.99 GFLOPS | Progress: (200/200) | 341.84 s Done.\n",
      "[Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1386, 16, 1), 'float32'), ('TENSOR', (1386,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1386, 16, 1), 'float32'), ('TENSOR', (1386,), 'int32'), ('TENSOR', (5,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (5535, 16, 1), 'float32'), ('TENSOR', (5535,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 28, 28, 128), 'float32'), ('TENSOR', (5535, 16, 1), 'float32'), ('TENSOR', (5535,), 'int32'), ('TENSOR', (9,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (22122, 16, 1), 'float32'), ('TENSOR', (22122,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 14, 14, 256), 'float32'), ('TENSOR', (22122, 16, 1), 'float32'), ('TENSOR', (22122,), 'int32'), ('TENSOR', (17,), 'int32'), 'NHWC')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv2d_NCHWc.x86, args=(('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32'), kwargs={}, workload=('conv2d_NCHWc.x86', ('TENSOR', (10, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'NCHW', 'float32')), Task(func_name=conv3x3_spNHWC.x86, args=(('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (88479, 16, 1), 'float32'), ('TENSOR', (88479,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC'), kwargs={}, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 7, 7, 512), 'float32'), ('TENSOR', (88479, 16, 1), 'float32'), ('TENSOR', (88479,), 'int32'), ('TENSOR', (33,), 'int32'), 'NHWC')), Task(func_name=dense_nopack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_nopack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32')), Task(func_name=dense_pack.x86, args=(('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'), kwargs={}, workload=('dense_pack.x86', ('TENSOR', (10, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'))]\n",
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:   65.52/ 198.06 GFLOPS | Progress: (200/200) | 455.89 s Done.\n",
      "conv3x3_spNHWC.x86 1944\n",
      " Current/Best:   74.84/ 127.15 GFLOPS | Progress: (200/200) | 342.13 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   42.65/ 185.15 GFLOPS | Progress: (200/200) | 295.56 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   78.97/ 146.69 GFLOPS | Progress: (200/200) | 293.90 s Done.\n",
      "conv3x3_spNHWC.x86 1512\n",
      " Current/Best:   56.71/ 142.92 GFLOPS | Progress: (200/200) | 317.79 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   44.86/ 170.91 GFLOPS | Progress: (200/200) | 338.59 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   19.98/ 182.81 GFLOPS | Progress: (200/200) | 290.50 s Done.\n",
      "conv3x3_spNHWC.x86 900\n",
      " Current/Best:   36.71/ 141.02 GFLOPS | Progress: (200/200) | 299.84 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:  138.93/ 161.68 GFLOPS | Progress: (200/200) | 338.83 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   51.27/ 181.07 GFLOPS | Progress: (200/200) | 288.35 s Done.\n",
      "conv3x3_spNHWC.x86 324\n",
      " Current/Best:   60.71/ 129.24 GFLOPS | Progress: (200/200) | 300.04 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   58.83/  61.21 GFLOPS | Progress: (200/200) | 315.41 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:    4.03/  74.01 GFLOPS | Progress: (200/200) | 326.87 s Done.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tvm import autotvm\n",
    "import random\n",
    "import numpy as np\n",
    "import onnx\n",
    "import logging\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "opts = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(timeout=20, min_repeat_ms=200),\n",
    ")\n",
    "\n",
    "for item in Path('sparse_resnet18_best_onnx').glob('resnet18_GL_16_PR_*_ckpt_best.onnx'):\n",
    "    onnx_model = onnx.load(str(item))\n",
    "    const_mod, params = relay.frontend.from_onnx(onnx_model, {'data': (10, 3, 224, 224)}, freeze_params=True)\n",
    "    const_mod2 = convert_mod_layout(const_mod, 'NHWC')\n",
    "    spconst_mod2 = convert_to_sparse(const_mod2, 'NHWC', 3, 16, 1, 0.09)\n",
    "    spconst_mod3 = convert_mod_layout(spconst_mod2, 'NCHW')\n",
    "    #print('\\n'.join(spconst_mod3.astext().splitlines()[:50]))\n",
    "    spconv_data = extract_spconv2d_data(spconst_mod3)\n",
    "    #print(spconv_data.keys())\n",
    "    #continue\n",
    "    \n",
    "    tasks = autotvm.task.extract_from_program(\n",
    "        spconst_mod3['main'], params={}, target='llvm -mcpu=cascadelake')\n",
    "    print(tasks)\n",
    "    for tsk in tasks:\n",
    "        opts['runner'].ref_input = None\n",
    "        if tsk.name.startswith('conv3x3_sp'):\n",
    "            data, wdat, wind, wptr, *attrs = tsk.args\n",
    "            weight = random.choice(spconv_data[data, wdat, wind, wptr])\n",
    "            wdat, wind, wptr = weight.data, weight.indices, weight.indptr\n",
    "            data = np.random.rand(*data[1]).astype(data[2])\n",
    "            ret = np.zeros_like(data)\n",
    "            opts['runner'].ref_input = [ret, wptr, wind, wdat, data]\n",
    "        print(tsk.name, len(tsk.config_space))\n",
    "        nsamples = min(200, len(tsk.config_space))\n",
    "        tuner = autotvm.tuner.GATuner(tsk)\n",
    "        tuner.tune(\n",
    "            nsamples,\n",
    "            measure_option=opts,\n",
    "            callbacks=[\n",
    "                autotvm.callback.progress_bar(nsamples),\n",
    "                autotvm.callback.log_to_file('test_sparse.log'),\n",
    "            ],\n",
    "        )\n",
    "    autotvm.record.pick_best('test_sparse.log', 'test_sparse.best.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18_GL_16_PR_0.3_ckpt_best.onnx\n",
      "dense 0.21441027519\n",
      "sparse 0.21627822932000001\n",
      "resnet18_GL_16_PR_0.8_ckpt_best.onnx\n",
      "dense 0.21331152976\n",
      "sparse 0.11990430067\n",
      "resnet18_GL_16_PR_0.6_ckpt_best.onnx\n",
      "dense 0.21217570862\n",
      "sparse 0.16341488359\n",
      "resnet18_GL_16_PR_0.1_ckpt_best.onnx\n",
      "dense 0.21691710173\n",
      "sparse 0.25188513607\n",
      "resnet18_GL_16_PR_0.9_ckpt_best.onnx\n",
      "dense 0.21254860435999998\n",
      "sparse 0.09621280157999999\n",
      "resnet18_GL_16_PR_0.7_ckpt_best.onnx\n",
      "dense 0.21140215602\n",
      "sparse 0.14273487242\n",
      "resnet18_GL_16_PR_0.5_ckpt_best.onnx\n",
      "dense 0.21587631831999998\n",
      "sparse 0.18474598572999998\n",
      "resnet18_GL_16_PR_0.2_ckpt_best.onnx\n",
      "dense 0.21202038227\n",
      "sparse 0.23153170424\n",
      "resnet18_GL_16_PR_0.4_ckpt_best.onnx\n",
      "dense 0.20996736257\n",
      "sparse 0.1965604406\n"
     ]
    }
   ],
   "source": [
    "from tvm import autotvm\n",
    "from tvm.contrib import graph_executor\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "import tvm\n",
    "\n",
    "dev = tvm.device('llvm -mcpu=cascadelake', 0)\n",
    "data = tvm.nd.array(np.random.rand(10, 3, 224, 224).astype('float32'))\n",
    "\n",
    "for item in Path('sparse_resnet18_best_onnx').glob('resnet18_GL_16_PR_*_ckpt_best.onnx'):\n",
    "    print(item.name)\n",
    "\n",
    "    onnx_model = onnx.load(str(item))\n",
    "    const_mod, params = relay.frontend.from_onnx(onnx_model, {'data': (10, 3, 224, 224)}, freeze_params=True)\n",
    "    const_mod2 = convert_mod_layout(const_mod, 'NHWC')\n",
    "    spconst_mod2 = convert_to_sparse(const_mod2, 'NHWC', 3, 16, 1, 0.09)\n",
    "    spconst_mod = convert_mod_layout(spconst_mod2, 'NCHW')\n",
    "\n",
    "    with autotvm.apply_history_best('test_dense.best.log'):\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            lib = relay.build_module.build(const_mod, params={}, target='llvm -mcpu=cascadelake')\n",
    "\n",
    "    graph_dense = graph_executor.create(lib.graph_json, lib.module, dev)\n",
    "    graph_dense.set_input(data=data, **lib.params)\n",
    "    ftimer = graph_dense.module.time_evaluator(\"run\", dev, number=100, repeat=1)\n",
    "    print('dense', ftimer().mean)\n",
    "\n",
    "    with autotvm.apply_history_best('test_sparse.best.log'):\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            lib = relay.build_module.build(spconst_mod, params={}, target='llvm -mcpu=cascadelake')\n",
    "\n",
    "    graph_sparse = graph_executor.create(lib.graph_json, lib.module, dev)\n",
    "    graph_sparse.set_input(data=data, **lib.params)\n",
    "    ftimer = graph_sparse.module.time_evaluator(\"run\", dev, number=100, repeat=1)\n",
    "    print('sparse', ftimer().mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-build",
   "language": "python",
   "name": "tvm-build"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
