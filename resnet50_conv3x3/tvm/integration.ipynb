{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topi test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import topi, testing, nd, target, te\n",
    "from sparse_utils import random_bsr_sparse\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "N, C, HW, VL, SP = 10, 64, 56, 16, 0.5\n",
    "NNZ = int(C * C * 9 // VL * SP)\n",
    "spweight = random_bsr_sparse((C, 9*C), (VL, 1), NNZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -keys=cpu -link-params=0 -mcpu=cascadelake, workload=('conv3x3_spNHWC.x86', ('TENSOR', (10, 56, 56, 64), 'float32'), ('TENSOR', (1152, 16, 1), 'float32'), ('TENSOR', (1152,), 'int32'), ('TENSOR', (5,), 'int32')). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHWC passed\n"
     ]
    }
   ],
   "source": [
    "# NHWC\n",
    "\n",
    "data = np.random.rand(N, HW, HW, C).astype('float32')\n",
    "weight = spweight.toarray().T.copy().reshape(3, 3, C, C)\n",
    "Data = te.placeholder(data.shape, 'float32')\n",
    "\n",
    "## dense\n",
    "\n",
    "Weight = te.placeholder(weight.shape, 'float32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.nn.conv2d_nhwc(Data, Weight, 1, 1, 1)\n",
    "    s = topi.generic.schedule_conv2d_nhwc(CC)\n",
    "    func = tvm.build(s, [Data, Weight, CC])\n",
    "args = [nd.array(data), nd.array(weight), nd.empty(CC.shape)]\n",
    "func(*args)\n",
    "\n",
    "## sparse\n",
    "\n",
    "Wdat = te.placeholder(spweight.data.shape, 'float32')\n",
    "Wind = te.placeholder(spweight.indices.shape, 'int32')\n",
    "Wptr = te.placeholder(spweight.indptr.shape, 'int32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.x86.sparse.spconv2d_3x3_nhwc(Data, Wdat, Wind, Wptr)\n",
    "    s = topi.x86.sparse.schedule_spconv2d_3x3_nhwc(CC)\n",
    "    func = tvm.build(s, [Data, Wdat, Wind, Wptr, CC])\n",
    "args2 = [nd.array(data), nd.array(spweight.data), nd.array(spweight.indices), nd.array(spweight.indptr), nd.empty(CC.shape)]\n",
    "func(*args2)\n",
    "\n",
    "## assert\n",
    "\n",
    "testing.assert_allclose(args[-1].numpy(), args2[-1].numpy())\n",
    "print('NHWC passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=llvm -keys=cpu -link-params=0 -mcpu=cascadelake, workload=('conv3x3_spNCHW.x86', ('TENSOR', (10, 64, 56, 56), 'float32'), ('TENSOR', (1152, 16, 1), 'float32'), ('TENSOR', (1152,), 'int32'), ('TENSOR', (5,), 'int32')). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCHW passed\n"
     ]
    }
   ],
   "source": [
    "# NCHW\n",
    "\n",
    "data = np.random.rand(N, C, HW, HW).astype('float32')\n",
    "weight = spweight.toarray().reshape(C, C, 3, 3)\n",
    "Data = te.placeholder(data.shape, 'float32')\n",
    "\n",
    "## dense\n",
    "\n",
    "Weight = te.placeholder(weight.shape, 'float32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.nn.conv2d_nchw(Data, Weight, 1, 1, 1, 'float32')\n",
    "    s = topi.generic.schedule_conv2d_nchw(CC)\n",
    "    func = tvm.build(s, [Data, Weight, CC])\n",
    "args = [nd.array(data), nd.array(weight), nd.empty(CC.shape)]\n",
    "func(*args)\n",
    "\n",
    "## sparse\n",
    "\n",
    "Wdat = te.placeholder(spweight.data.shape, 'float32')\n",
    "Wind = te.placeholder(spweight.indices.shape, 'int32')\n",
    "Wptr = te.placeholder(spweight.indptr.shape, 'int32')\n",
    "with target.Target('llvm -mcpu=cascadelake'):\n",
    "    CC = topi.x86.sparse.spconv2d_3x3_nchw(Data, Wdat, Wind, Wptr)\n",
    "    s = topi.x86.sparse.schedule_spconv2d_3x3_nchw(CC)\n",
    "    func = tvm.build(s, [Data, Wdat, Wind, Wptr, CC])\n",
    "args2 = [nd.array(data), nd.array(spweight.data), nd.array(spweight.indices), nd.array(spweight.indptr), nd.empty(CC.shape)]\n",
    "func(*args2)\n",
    "\n",
    "## assert\n",
    "\n",
    "testing.assert_allclose(args[-1].numpy(), args2[-1].numpy())\n",
    "print('NCHW passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS 1\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"sparse_resnet18_best_onnx/resnet18_GL_16_PR_0.6_ckpt_best.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import relay\n",
    "const_mod, params = relay.frontend.from_onnx(onnx_model, {'data': (10, 3, 224, 224)}, freeze_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) {',\n",
       " '  %0 = nn.conv2d(%data, meta[relay.Constant][0], strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7]);',\n",
       " '  %1 = nn.bias_add(%0, meta[relay.Constant][1]);',\n",
       " '  %2 = nn.relu(%1);',\n",
       " '  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);',\n",
       " '  %4 = nn.conv2d(%3, meta[relay.Constant][2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %5 = nn.bias_add(%4, meta[relay.Constant][3]);',\n",
       " '  %6 = nn.relu(%5);',\n",
       " '  %7 = nn.conv2d(%6, meta[relay.Constant][4], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %8 = nn.bias_add(%7, meta[relay.Constant][5]);',\n",
       " '  %9 = add(%8, %3);',\n",
       " '  %10 = nn.relu(%9);',\n",
       " '  %11 = nn.conv2d(%10, meta[relay.Constant][6], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %12 = nn.bias_add(%11, meta[relay.Constant][7]);',\n",
       " '  %13 = nn.relu(%12);',\n",
       " '  %14 = nn.conv2d(%13, meta[relay.Constant][8], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %15 = nn.bias_add(%14, meta[relay.Constant][9]);',\n",
       " '  %16 = add(%15, %10);',\n",
       " '  %17 = nn.relu(%16);',\n",
       " '  %18 = nn.conv2d(%17, meta[relay.Constant][10], strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %19 = nn.bias_add(%18, meta[relay.Constant][11]);',\n",
       " '  %20 = nn.relu(%19);',\n",
       " '  %21 = nn.conv2d(%20, meta[relay.Constant][12], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %22 = nn.conv2d(%17, meta[relay.Constant][14], strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]);',\n",
       " '  %23 = nn.bias_add(%21, meta[relay.Constant][13]);',\n",
       " '  %24 = nn.bias_add(%22, meta[relay.Constant][15]);',\n",
       " '  %25 = add(%23, %24);',\n",
       " '  %26 = nn.relu(%25);',\n",
       " '  %27 = nn.conv2d(%26, meta[relay.Constant][16], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %28 = nn.bias_add(%27, meta[relay.Constant][17]);',\n",
       " '  %29 = nn.relu(%28);',\n",
       " '  %30 = nn.conv2d(%29, meta[relay.Constant][18], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %31 = nn.bias_add(%30, meta[relay.Constant][19]);',\n",
       " '  %32 = add(%31, %26);',\n",
       " '  %33 = nn.relu(%32);',\n",
       " '  %34 = nn.conv2d(%33, meta[relay.Constant][20], strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %35 = nn.bias_add(%34, meta[relay.Constant][21]);',\n",
       " '  %36 = nn.relu(%35);',\n",
       " '  %37 = nn.conv2d(%36, meta[relay.Constant][22], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %38 = nn.conv2d(%33, meta[relay.Constant][24], strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]);',\n",
       " '  %39 = nn.bias_add(%37, meta[relay.Constant][23]);',\n",
       " '  %40 = nn.bias_add(%38, meta[relay.Constant][25]);',\n",
       " '  %41 = add(%39, %40);',\n",
       " '  %42 = nn.relu(%41);',\n",
       " '  %43 = nn.conv2d(%42, meta[relay.Constant][26], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %44 = nn.bias_add(%43, meta[relay.Constant][27]);',\n",
       " '  %45 = nn.relu(%44);',\n",
       " '  %46 = nn.conv2d(%45, meta[relay.Constant][28], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %47 = nn.bias_add(%46, meta[relay.Constant][29]);',\n",
       " '  %48 = add(%47, %42);',\n",
       " '  %49 = nn.relu(%48);',\n",
       " '  %50 = nn.conv2d(%49, meta[relay.Constant][30], strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %51 = nn.bias_add(%50, meta[relay.Constant][31]);',\n",
       " '  %52 = nn.relu(%51);',\n",
       " '  %53 = nn.conv2d(%52, meta[relay.Constant][32], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %54 = nn.conv2d(%49, meta[relay.Constant][34], strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]);',\n",
       " '  %55 = nn.bias_add(%53, meta[relay.Constant][33]);',\n",
       " '  %56 = nn.bias_add(%54, meta[relay.Constant][35]);',\n",
       " '  %57 = add(%55, %56);',\n",
       " '  %58 = nn.relu(%57);',\n",
       " '  %59 = nn.conv2d(%58, meta[relay.Constant][36], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %60 = nn.bias_add(%59, meta[relay.Constant][37]);',\n",
       " '  %61 = nn.relu(%60);',\n",
       " '  %62 = nn.conv2d(%61, meta[relay.Constant][38], padding=[1, 1, 1, 1], kernel_size=[3, 3]);',\n",
       " '  %63 = nn.bias_add(%62, meta[relay.Constant][39]);',\n",
       " '  %64 = add(%63, %58);',\n",
       " '  %65 = nn.relu(%64);',\n",
       " '  %66 = nn.global_avg_pool2d(%65);',\n",
       " '  %67 = reshape(%66, newshape=[10, -1]);',\n",
       " '  %68 = nn.batch_flatten(%67);',\n",
       " '  %69 = nn.dense(%68, meta[relay.Constant][40], units=1000);',\n",
       " '  add(%69, meta[relay.Constant][41])',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ',\n",
       " '      \"data\": [2]',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Array\", ',\n",
       " '      \"data\": [',\n",
       " '        3, ',\n",
       " '        4, ',\n",
       " '        5, ',\n",
       " '        6, ',\n",
       " '        7, ',\n",
       " '        8, ',\n",
       " '        9, ',\n",
       " '        10, ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_mod.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import autotvm\n",
    "\n",
    "tasks = autotvm.task.extract_from_program(\n",
    "    const_mod['main'], params={}, target='llvm -mcpu=cascadelake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:  115.19/ 197.50 GFLOPS | Progress: (200/200) | 448.85 s Done.\n",
      "conv2d_NCHWc.x86 980\n",
      " Current/Best:    9.94/ 203.54 GFLOPS | Progress: (200/200) | 390.84 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:    7.72/ 183.42 GFLOPS | Progress: (200/200) | 328.03 s Done.\n",
      "conv2d_NCHWc.x86 896\n",
      " Current/Best:   68.05/ 143.04 GFLOPS | Progress: (200/200) | 290.39 s Done.\n",
      "conv2d_NCHWc.x86 1024\n",
      " Current/Best:   30.71/ 212.58 GFLOPS | Progress: (200/200) | 397.88 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   40.97/ 178.58 GFLOPS | Progress: (200/200) | 312.93 s Done.\n",
      "conv2d_NCHWc.x86 864\n",
      " Current/Best:   43.79/ 166.37 GFLOPS | Progress: (200/200) | 286.90 s Done.\n",
      "conv2d_NCHWc.x86 972\n",
      " Current/Best:  144.97/ 211.71 GFLOPS | Progress: (200/200) | 443.07 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   64.33/ 159.66 GFLOPS | Progress: (200/200) | 367.98 s Done.\n",
      "conv2d_NCHWc.x86 720\n",
      " Current/Best:   89.31/ 181.01 GFLOPS | Progress: (200/200) | 278.24 s Done.\n",
      "conv2d_NCHWc.x86 800\n",
      " Current/Best:   97.93/ 178.50 GFLOPS | Progress: (200/200) | 497.59 s Done.\n",
      "dense_nopack.x86 640\n",
      " Current/Best:   41.85/  63.25 GFLOPS | Progress: (200/200) | 299.72 s Done.\n",
      "dense_pack.x86 36000\n",
      " Current/Best:    7.83/  91.29 GFLOPS | Progress: (200/200) | 334.28 s Done.\n"
     ]
    }
   ],
   "source": [
    "opts = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(timeout=20, min_repeat_ms=200),\n",
    ")\n",
    "\n",
    "for tsk in tasks:\n",
    "    print(tsk.name, len(tsk.config_space))\n",
    "    nsamples = min(200, len(tsk.config_space))\n",
    "    tuner = autotvm.tuner.GATuner(tsk)\n",
    "    tuner.tune(\n",
    "        nsamples,\n",
    "        measure_option=opts,\n",
    "        callbacks=[\n",
    "            autotvm.callback.progress_bar(nsamples),\n",
    "            autotvm.callback.log_to_file('test_dense.log'),\n",
    "        ],\n",
    "    )\n",
    "autotvm.record.pick_best('test_dense.log', 'test_dense.best.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Name                                                        Ops                                                             Time(us)    Time(%)  Shape                  Inputs  Outputs  \n",
      "---------                                                        ---                                                             --------    -------  -----                  ------  -------  \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu         tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu        16015.9     7.553    (10, 2, 112, 112, 32)  3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_31  tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3  15272.8     7.203    (10, 16, 7, 7, 32)     4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7      15067.0     7.106    (10, 16, 7, 7, 32)     3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3   tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_3  14927.2     7.04     (10, 16, 7, 7, 32)     4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu     tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu    12448.3     5.871    (10, 1, 56, 56, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      12292.0     5.797    (10, 1, 56, 56, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu1    tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu    12226.6     5.766    (10, 1, 56, 56, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      11921.9     5.622    (10, 1, 56, 56, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1   tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1  11597.0     5.469    (10, 2, 28, 28, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2   tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2  11557.1     5.45     (10, 8, 14, 14, 32)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_21  tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_2  11519.3     5.432    (10, 8, 14, 14, 32)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5      11467.9     5.408    (10, 8, 14, 14, 32)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_11  tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1  11465.0     5.407    (10, 2, 28, 28, 64)    4       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3      11273.8     5.317    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6      7119.42     3.357    (10, 16, 7, 7, 32)     3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2      6949.68     3.277    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4       tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4      6840.68     3.226    (10, 4, 14, 14, 64)    3       1        \n",
      "tvmgen_default_fused_nn_max_pool2d                               tvmgen_default_fused_nn_max_pool2d                              3213.29     1.515    (10, 2, 56, 56, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_1                          tvmgen_default_fused_layout_transform_1                         1127.14     0.532    (10, 1, 56, 56, 64)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                 tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                1073.58     0.506    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_layout_transform                            tvmgen_default_fused_layout_transform                           952.402     0.449    (10, 3, 224, 224, 1)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2               tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2              946.026     0.446    (10, 8, 7, 7, 64)      3       1        \n",
      "tvmgen_default_fused_layout_transform_2                          tvmgen_default_fused_layout_transform_2                         867.997     0.409    (10, 2, 56, 56, 32)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1               tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1              830.202     0.392    (10, 8, 14, 14, 32)    3       1        \n",
      "tvmgen_default_fused_layout_transform_3                          tvmgen_default_fused_layout_transform_3                         411.533     0.194    (10, 8, 28, 28, 16)    1       1        \n",
      "tvmgen_default_fused_layout_transform_31                         tvmgen_default_fused_layout_transform_3                         359.782     0.17     (10, 8, 28, 28, 16)    1       1        \n",
      "tvmgen_default_fused_layout_transform_32                         tvmgen_default_fused_layout_transform_3                         335.402     0.158    (10, 8, 28, 28, 16)    1       1        \n",
      "tvmgen_default_fused_layout_transform_4                          tvmgen_default_fused_layout_transform_4                         303.86      0.143    (10, 4, 28, 28, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_5                          tvmgen_default_fused_layout_transform_5                         302.23      0.143    (10, 1, 28, 28, 128)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_dense_pack_add                   tvmgen_default_fused_nn_contrib_dense_pack_add                  230.737     0.109    (10, 1000)             3       1        \n",
      "tvmgen_default_fused_layout_transform_9                          tvmgen_default_fused_layout_transform_9                         162.804     0.077    (10, 2, 14, 14, 128)   1       1        \n",
      "tvmgen_default_fused_layout_transform_61                         tvmgen_default_fused_layout_transform_6                         156.1       0.074    (10, 4, 14, 14, 64)    1       1        \n",
      "tvmgen_default_fused_layout_transform_6                          tvmgen_default_fused_layout_transform_6                         154.965     0.073    (10, 4, 14, 14, 64)    1       1        \n",
      "tvmgen_default_fused_layout_transform_7                          tvmgen_default_fused_layout_transform_7                         154.649     0.073    (10, 1, 14, 14, 256)   1       1        \n",
      "tvmgen_default_fused_layout_transform_10                         tvmgen_default_fused_layout_transform_10                        113.334     0.053    (10, 16, 7, 7, 32)     1       1        \n",
      "tvmgen_default_fused_nn_global_avg_pool2d                        tvmgen_default_fused_nn_global_avg_pool2d                       103.965     0.049    (10, 16, 1, 1, 32)     1       1        \n",
      "tvmgen_default_fused_layout_transform_81                         tvmgen_default_fused_layout_transform_8                         101.994     0.048    (10, 32, 7, 7, 16)     1       1        \n",
      "tvmgen_default_fused_layout_transform_82                         tvmgen_default_fused_layout_transform_8                         94.371      0.045    (10, 32, 7, 7, 16)     1       1        \n",
      "tvmgen_default_fused_layout_transform_8                          tvmgen_default_fused_layout_transform_8                         84.365      0.04     (10, 32, 7, 7, 16)     1       1        \n",
      "tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten   tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten  3.399       0.002    (10, 512)              1       1        \n",
      "Total_time                                                       -                                                               212045.707  -        -                      -       -        \n"
     ]
    }
   ],
   "source": [
    "from tvm import autotvm\n",
    "from tvm.contrib.debugger import debug_executor\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "with autotvm.apply_history_best('test_dense.best.log'):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build_module.build(const_mod, params={}, target='llvm -mcpu=cascadelake')\n",
    "\n",
    "dev = tvm.device('llvm -mcpu=cascadelake', 0)\n",
    "data = tvm.nd.array(np.random.rand(10, 3, 224, 224).astype('float32'))\n",
    "\n",
    "graph_dense = debug_executor.create(lib.graph_json, lib.module, dev)\n",
    "graph_dense.set_input(data=data, **lib.params)\n",
    "\n",
    "graph_dense.run()\n",
    "#ftimer = graph_dense.module.time_evaluator(\"run\", dev, number=100, repeat=1)\n",
    "#ftimer().mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfunc = relay.data_dep_optimization.utils._run_opt_pass(\n",
    "    const_mod['main'],\n",
    "    relay.transform._ffi_api.Conv2dToSparse2(\"NCHW\", 3, 16, 1, 0.4)\n",
    ")\n",
    "spconst_mod = tvm.ir.IRModule.from_expr(newfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) -> Tensor[(10, 1000), float32] {',\n",
       " '  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7]) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %2 = nn.relu(%1) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %4 = nn.sparse_conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(927), int32] */, meta[relay.Constant][4] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %5 = nn.bias_add(%4, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %6 = nn.relu(%5) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %7 = nn.sparse_conv2d(%6, meta[relay.Constant][6] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][7] /* ty=Tensor[(927), int32] */, meta[relay.Constant][8] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %8 = nn.bias_add(%7, meta[relay.Constant][9] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %9 = add(%8, %3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %10 = nn.relu(%9) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %11 = nn.sparse_conv2d(%10, meta[relay.Constant][10] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(927), int32] */, meta[relay.Constant][12] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %12 = nn.bias_add(%11, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %13 = nn.relu(%12) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %14 = nn.sparse_conv2d(%13, meta[relay.Constant][14] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][15] /* ty=Tensor[(927), int32] */, meta[relay.Constant][16] /* ty=Tensor[(5), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %15 = nn.bias_add(%14, meta[relay.Constant][17] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %16 = add(%15, %10) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %17 = nn.relu(%16) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %18 = nn.conv2d(%17, meta[relay.Constant][18] /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %19 = nn.bias_add(%18, meta[relay.Constant][19] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %20 = nn.relu(%19) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %21 = nn.sparse_conv2d(%20, meta[relay.Constant][20] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][22] /* ty=Tensor[(9), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %22 = nn.conv2d(%17, meta[relay.Constant][24] /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %23 = nn.bias_add(%21, meta[relay.Constant][23] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %24 = nn.bias_add(%22, meta[relay.Constant][25] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %25 = add(%23, %24) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %26 = nn.relu(%25) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %27 = nn.sparse_conv2d(%26, meta[relay.Constant][26] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][27] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][28] /* ty=Tensor[(9), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %28 = nn.bias_add(%27, meta[relay.Constant][29] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %29 = nn.relu(%28) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %30 = nn.sparse_conv2d(%29, meta[relay.Constant][30] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][32] /* ty=Tensor[(9), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %31 = nn.bias_add(%30, meta[relay.Constant][33] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %32 = add(%31, %26) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %33 = nn.relu(%32) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %34 = nn.conv2d(%33, meta[relay.Constant][34] /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %35 = nn.bias_add(%34, meta[relay.Constant][35] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %36 = nn.relu(%35) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %37 = nn.sparse_conv2d(%36, meta[relay.Constant][36] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][37] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][38] /* ty=Tensor[(17), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %38 = nn.conv2d(%33, meta[relay.Constant][40] /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %39 = nn.bias_add(%37, meta[relay.Constant][39] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %40 = nn.bias_add(%38, meta[relay.Constant][41] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %41 = add(%39, %40) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %42 = nn.relu(%41) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %43 = nn.sparse_conv2d(%42, meta[relay.Constant][42] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][43] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][44] /* ty=Tensor[(17), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %44 = nn.bias_add(%43, meta[relay.Constant][45] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %45 = nn.relu(%44) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %46 = nn.sparse_conv2d(%45, meta[relay.Constant][46] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][47] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][48] /* ty=Tensor[(17), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %47 = nn.bias_add(%46, meta[relay.Constant][49] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %48 = add(%47, %42) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %49 = nn.relu(%48) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %50 = nn.conv2d(%49, meta[relay.Constant][50] /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %51 = nn.bias_add(%50, meta[relay.Constant][51] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %52 = nn.relu(%51) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %53 = nn.sparse_conv2d(%52, meta[relay.Constant][52] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][53] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][54] /* ty=Tensor[(33), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %54 = nn.conv2d(%49, meta[relay.Constant][56] /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %55 = nn.bias_add(%53, meta[relay.Constant][55] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %56 = nn.bias_add(%54, meta[relay.Constant][57] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %57 = add(%55, %56) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %58 = nn.relu(%57) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %59 = nn.sparse_conv2d(%58, meta[relay.Constant][58] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][59] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][60] /* ty=Tensor[(33), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %60 = nn.bias_add(%59, meta[relay.Constant][61] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %61 = nn.relu(%60) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %62 = nn.sparse_conv2d(%61, meta[relay.Constant][62] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][63] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][64] /* ty=Tensor[(33), int32] */, layout=\"NCHW\", kernel_size=3) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %63 = nn.bias_add(%62, meta[relay.Constant][65] /* ty=Tensor[(512), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %64 = add(%63, %58) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %65 = nn.relu(%64) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %66 = nn.global_avg_pool2d(%65) /* ty=Tensor[(10, 512, 1, 1), float32] */;',\n",
       " '  %67 = reshape(%66, newshape=[10, -1]) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %68 = nn.batch_flatten(%67) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %69 = nn.dense(%68, meta[relay.Constant][66] /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(10, 1000), float32] */;',\n",
       " '  add(%69, meta[relay.Constant][67] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(10, 1000), float32] */',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ',\n",
       " '      \"data\": [2]',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Array\", ',\n",
       " '      \"data\": [',\n",
       " '        3, ',\n",
       " '        10, ',\n",
       " '        13, ',\n",
       " '        19, ',\n",
       " '        23, ',\n",
       " '        27, ',\n",
       " '        31, ',\n",
       " '        37, ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spconst_mod.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_layouts = {'nn.conv2d': ['NHWC', 'default']}\n",
    "seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                relay.transform.ConvertLayout(desired_layouts),\n",
    "                                relay.transform.FoldConstant()])\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    const_mod2 = seq(const_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfunc = relay.data_dep_optimization.utils._run_opt_pass(\n",
    "    const_mod2['main'],\n",
    "    relay.transform._ffi_api.Conv2dToSparse2(\"NHWC\", 3, 16, 1, 0.4)\n",
    ")\n",
    "spconst_mod2 = tvm.ir.IRModule.from_expr(newfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) -> Tensor[(10, 1000), float32] {',\n",
       " '  %0 = layout_transform(%data, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 224, 224, 3), float32] */;',\n",
       " '  %1 = nn.conv2d(%0, meta[relay.Constant][0] /* ty=Tensor[(7, 7, 3, 64), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 112, 112, 64), float32] */;',\n",
       " '  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 112, 112, 64), float32] */;',\n",
       " '  %3 = nn.relu(%2) /* ty=Tensor[(10, 112, 112, 64), float32] */;',\n",
       " '  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1], layout=\"NHWC\") /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %5 = nn.sparse_conv2d(%4, meta[relay.Constant][2] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(927), int32] */, meta[relay.Constant][4] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %6 = add(%5, meta[relay.Constant][5] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %7 = nn.relu(%6) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %8 = nn.sparse_conv2d(%7, meta[relay.Constant][6] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][7] /* ty=Tensor[(927), int32] */, meta[relay.Constant][8] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %9 = add(%8, meta[relay.Constant][9] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %10 = add(%9, %4) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %11 = nn.relu(%10) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %12 = nn.sparse_conv2d(%11, meta[relay.Constant][10] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(927), int32] */, meta[relay.Constant][12] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %13 = add(%12, meta[relay.Constant][13] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %14 = nn.relu(%13) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %15 = nn.sparse_conv2d(%14, meta[relay.Constant][14] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][15] /* ty=Tensor[(927), int32] */, meta[relay.Constant][16] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %16 = add(%15, meta[relay.Constant][17] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %17 = add(%16, %11) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %18 = nn.relu(%17) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %19 = nn.conv2d(%18, meta[relay.Constant][18] /* ty=Tensor[(3, 3, 64, 128), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %20 = add(%19, meta[relay.Constant][19] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %21 = nn.relu(%20) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %22 = nn.sparse_conv2d(%21, meta[relay.Constant][20] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][22] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %23 = nn.conv2d(%18, meta[relay.Constant][24] /* ty=Tensor[(1, 1, 64, 128), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %24 = add(%22, meta[relay.Constant][23] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %25 = add(%23, meta[relay.Constant][25] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %26 = add(%24, %25) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %27 = nn.relu(%26) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %28 = nn.sparse_conv2d(%27, meta[relay.Constant][26] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][27] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][28] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %29 = add(%28, meta[relay.Constant][29] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %30 = nn.relu(%29) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %31 = nn.sparse_conv2d(%30, meta[relay.Constant][30] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][32] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %32 = add(%31, meta[relay.Constant][33] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %33 = add(%32, %27) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %34 = nn.relu(%33) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %35 = nn.conv2d(%34, meta[relay.Constant][34] /* ty=Tensor[(3, 3, 128, 256), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %36 = add(%35, meta[relay.Constant][35] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %37 = nn.relu(%36) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %38 = nn.sparse_conv2d(%37, meta[relay.Constant][36] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][37] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][38] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %39 = nn.conv2d(%34, meta[relay.Constant][40] /* ty=Tensor[(1, 1, 128, 256), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %40 = add(%38, meta[relay.Constant][39] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %41 = add(%39, meta[relay.Constant][41] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %42 = add(%40, %41) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %43 = nn.relu(%42) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %44 = nn.sparse_conv2d(%43, meta[relay.Constant][42] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][43] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][44] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %45 = add(%44, meta[relay.Constant][45] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %46 = nn.relu(%45) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %47 = nn.sparse_conv2d(%46, meta[relay.Constant][46] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][47] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][48] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %48 = add(%47, meta[relay.Constant][49] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %49 = add(%48, %43) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %50 = nn.relu(%49) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %51 = nn.conv2d(%50, meta[relay.Constant][50] /* ty=Tensor[(3, 3, 256, 512), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %52 = add(%51, meta[relay.Constant][51] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %53 = nn.relu(%52) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %54 = nn.sparse_conv2d(%53, meta[relay.Constant][52] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][53] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][54] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %55 = nn.conv2d(%50, meta[relay.Constant][56] /* ty=Tensor[(1, 1, 256, 512), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1], data_layout=\"NHWC\", kernel_layout=\"HWIO\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %56 = add(%54, meta[relay.Constant][55] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %57 = add(%55, meta[relay.Constant][57] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %58 = add(%56, %57) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %59 = nn.relu(%58) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %60 = nn.sparse_conv2d(%59, meta[relay.Constant][58] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][59] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][60] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %61 = add(%60, meta[relay.Constant][61] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %62 = nn.relu(%61) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %63 = nn.sparse_conv2d(%62, meta[relay.Constant][62] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][63] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][64] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %64 = add(%63, meta[relay.Constant][65] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %65 = add(%64, %59) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %66 = nn.relu(%65) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %67 = nn.global_avg_pool2d(%66, layout=\"NHWC\") /* ty=Tensor[(10, 1, 1, 512), float32] */;',\n",
       " '  %68 = layout_transform(%67, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 512, 1, 1), float32] */;',\n",
       " '  %69 = reshape(%68, newshape=[10, -1]) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %70 = nn.batch_flatten(%69) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %71 = nn.dense(%70, meta[relay.Constant][66] /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(10, 1000), float32] */;',\n",
       " '  add(%71, meta[relay.Constant][67] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(10, 1000), float32] */',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ',\n",
       " '      \"data\": [2]',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Array\", ',\n",
       " '      \"data\": [',\n",
       " '        3, ',\n",
       " '        10, ',\n",
       " '        17, ',\n",
       " '        23, ',\n",
       " '        27, ',\n",
       " '        31, ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spconst_mod2.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_layouts = {'nn.conv2d': ['NCHW', 'default']}\n",
    "seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                relay.transform.ConvertLayout(desired_layouts),\n",
    "                                relay.transform.FoldConstant()])\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    spconst_mod3 = seq(spconst_mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#[version = \"0.0.5\"]',\n",
       " 'def @main(%data: Tensor[(10, 3, 224, 224), float32]) -> Tensor[(10, 1000), float32] {',\n",
       " '  %0 = layout_transform(%data, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 224, 224, 3), float32] */;',\n",
       " '  %1 = layout_transform(%0, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 3, 224, 224), float32] */;',\n",
       " '  %2 = nn.conv2d(%1, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], kernel_size=[7, 7]) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %3 = add(%2, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 1, 1), float32] */) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %4 = nn.relu(%3) /* ty=Tensor[(10, 64, 112, 112), float32] */;',\n",
       " '  %5 = nn.max_pool2d(%4, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %6 = layout_transform(%5, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %7 = nn.sparse_conv2d(%6, meta[relay.Constant][2] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(927), int32] */, meta[relay.Constant][4] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %8 = add(%7, meta[relay.Constant][5] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %9 = nn.relu(%8) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %10 = nn.sparse_conv2d(%9, meta[relay.Constant][6] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][7] /* ty=Tensor[(927), int32] */, meta[relay.Constant][8] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %11 = add(%10, meta[relay.Constant][9] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %12 = add(%11, %6) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %13 = nn.relu(%12) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %14 = nn.sparse_conv2d(%13, meta[relay.Constant][10] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(927), int32] */, meta[relay.Constant][12] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %15 = add(%14, meta[relay.Constant][13] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %16 = nn.relu(%15) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %17 = nn.sparse_conv2d(%16, meta[relay.Constant][14] /* ty=Tensor[(927, 16, 1), float32] */, meta[relay.Constant][15] /* ty=Tensor[(927), int32] */, meta[relay.Constant][16] /* ty=Tensor[(5), int32] */, kernel_size=3) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %18 = add(%17, meta[relay.Constant][17] /* ty=Tensor[(1, 1, 1, 64), float32] */) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %19 = add(%18, %13) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %20 = nn.relu(%19) /* ty=Tensor[(10, 56, 56, 64), float32] */;',\n",
       " '  %21 = layout_transform(%20, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 64, 56, 56), float32] */;',\n",
       " '  %22 = nn.conv2d(%21, meta[relay.Constant][18] /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %23 = add(%22, meta[relay.Constant][19] /* ty=Tensor[(1, 128, 1, 1), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %24 = nn.relu(%23) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %25 = layout_transform(%24, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %26 = nn.sparse_conv2d(%25, meta[relay.Constant][20] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][22] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %27 = nn.conv2d(%21, meta[relay.Constant][24] /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %28 = add(%27, meta[relay.Constant][25] /* ty=Tensor[(1, 128, 1, 1), float32] */) /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %29 = add(%26, meta[relay.Constant][23] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %30 = layout_transform(%28, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %31 = add(%29, %30) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %32 = nn.relu(%31) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %33 = nn.sparse_conv2d(%32, meta[relay.Constant][26] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][27] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][28] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %34 = add(%33, meta[relay.Constant][29] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %35 = nn.relu(%34) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %36 = nn.sparse_conv2d(%35, meta[relay.Constant][30] /* ty=Tensor[(3690, 16, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(3690), int32] */, meta[relay.Constant][32] /* ty=Tensor[(9), int32] */, kernel_size=3) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %37 = add(%36, meta[relay.Constant][33] /* ty=Tensor[(1, 1, 1, 128), float32] */) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %38 = add(%37, %32) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %39 = nn.relu(%38) /* ty=Tensor[(10, 28, 28, 128), float32] */;',\n",
       " '  %40 = layout_transform(%39, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 128, 28, 28), float32] */;',\n",
       " '  %41 = nn.conv2d(%40, meta[relay.Constant][34] /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %42 = add(%41, meta[relay.Constant][35] /* ty=Tensor[(1, 256, 1, 1), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %43 = nn.relu(%42) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %44 = layout_transform(%43, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %45 = nn.sparse_conv2d(%44, meta[relay.Constant][36] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][37] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][38] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %46 = nn.conv2d(%40, meta[relay.Constant][40] /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %47 = add(%46, meta[relay.Constant][41] /* ty=Tensor[(1, 256, 1, 1), float32] */) /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %48 = add(%45, meta[relay.Constant][39] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %49 = layout_transform(%47, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %50 = add(%48, %49) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %51 = nn.relu(%50) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %52 = nn.sparse_conv2d(%51, meta[relay.Constant][42] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][43] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][44] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %53 = add(%52, meta[relay.Constant][45] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %54 = nn.relu(%53) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %55 = nn.sparse_conv2d(%54, meta[relay.Constant][46] /* ty=Tensor[(14751, 16, 1), float32] */, meta[relay.Constant][47] /* ty=Tensor[(14751), int32] */, meta[relay.Constant][48] /* ty=Tensor[(17), int32] */, kernel_size=3) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %56 = add(%55, meta[relay.Constant][49] /* ty=Tensor[(1, 1, 1, 256), float32] */) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %57 = add(%56, %51) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %58 = nn.relu(%57) /* ty=Tensor[(10, 14, 14, 256), float32] */;',\n",
       " '  %59 = layout_transform(%58, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 256, 14, 14), float32] */;',\n",
       " '  %60 = nn.conv2d(%59, meta[relay.Constant][50] /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], kernel_size=[3, 3]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %61 = add(%60, meta[relay.Constant][51] /* ty=Tensor[(1, 512, 1, 1), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %62 = nn.relu(%61) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %63 = layout_transform(%62, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %64 = nn.sparse_conv2d(%63, meta[relay.Constant][52] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][53] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][54] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %65 = nn.conv2d(%59, meta[relay.Constant][56] /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], kernel_size=[1, 1]) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %66 = add(%65, meta[relay.Constant][57] /* ty=Tensor[(1, 512, 1, 1), float32] */) /* ty=Tensor[(10, 512, 7, 7), float32] */;',\n",
       " '  %67 = add(%64, meta[relay.Constant][55] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %68 = layout_transform(%66, src_layout=\"NCHW\", dst_layout=\"NHWC\") /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %69 = add(%67, %68) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %70 = nn.relu(%69) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %71 = nn.sparse_conv2d(%70, meta[relay.Constant][58] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][59] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][60] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %72 = add(%71, meta[relay.Constant][61] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %73 = nn.relu(%72) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %74 = nn.sparse_conv2d(%73, meta[relay.Constant][62] /* ty=Tensor[(58986, 16, 1), float32] */, meta[relay.Constant][63] /* ty=Tensor[(58986), int32] */, meta[relay.Constant][64] /* ty=Tensor[(33), int32] */, kernel_size=3) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %75 = add(%74, meta[relay.Constant][65] /* ty=Tensor[(1, 1, 1, 512), float32] */) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %76 = add(%75, %70) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %77 = nn.relu(%76) /* ty=Tensor[(10, 7, 7, 512), float32] */;',\n",
       " '  %78 = nn.global_avg_pool2d(%77, layout=\"NHWC\") /* ty=Tensor[(10, 1, 1, 512), float32] */;',\n",
       " '  %79 = layout_transform(%78, src_layout=\"NHWC\", dst_layout=\"NCHW\") /* ty=Tensor[(10, 512, 1, 1), float32] */;',\n",
       " '  %80 = reshape(%79, newshape=[10, -1]) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %81 = nn.batch_flatten(%80) /* ty=Tensor[(10, 512), float32] */;',\n",
       " '  %82 = nn.dense(%81, meta[relay.Constant][66] /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(10, 1000), float32] */;',\n",
       " '  add(%82, meta[relay.Constant][67] /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(10, 1000), float32] */',\n",
       " '}',\n",
       " '',\n",
       " '#[metadata]',\n",
       " '{',\n",
       " '  \"root\": 1, ',\n",
       " '  \"nodes\": [',\n",
       " '    {',\n",
       " '      \"type_key\": \"\"',\n",
       " '    }, ',\n",
       " '    {',\n",
       " '      \"type_key\": \"Map\", ',\n",
       " '      \"keys\": [',\n",
       " '        \"relay.Constant\"',\n",
       " '      ], ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spconst_mod3.astext().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import autotvm\n",
    "\n",
    "tasks = autotvm.task.extract_from_program(\n",
    "    spconst_mod['main'], params={}, target='llvm -mcpu=cascadelake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "spconv_data = {}\n",
    "\n",
    "def fvisit(e):\n",
    "    if isinstance(e, relay.Call) and e.op.name == 'nn.sparse_conv2d':\n",
    "        args_type = [i.checked_type for i in e.args]\n",
    "        args_type = tuple(('TENSOR', i.concrete_shape, i.dtype) for i in args_type)\n",
    "        weight = tuple(i.data.numpy() for i in e.args[1:])\n",
    "        weight = sparse.bsr_matrix(weight)\n",
    "        spconv_data.setdefault(args_type, []).append(weight)\n",
    "\n",
    "relay.analysis.post_order_visit(spconst_mod['main'], fvisit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_NCHWc.x86 308\n",
      " Current/Best:    0.00/   0.00 GFLOPS | Progress: (0/308) | 0.00 s"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "opts = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(timeout=20, min_repeat_ms=200),\n",
    ")\n",
    "\n",
    "for tsk in tasks:\n",
    "    opts['runner'].ref_input = None\n",
    "    if tsk.name.startswith('conv3x3_sp'):\n",
    "        data, wdat, wind, wptr, *attrs = tsk.args\n",
    "        weight = random.choice(spconv_data[data, wdat, wind, wptr])\n",
    "        wdat, wind, wptr = weight.data, weight.indices, weight.indptr\n",
    "        data = np.random.rand(*data[1]).astype(data[2])\n",
    "        ret = np.zeros_like(data)\n",
    "        opts['runner'].ref_input = [ret, wptr, wind, wdat, data]\n",
    "    print(tsk.name, len(tsk.config_space))\n",
    "    nsamples = min(1000, len(tsk.config_space))\n",
    "    tuner = autotvm.tuner.GATuner(tsk)\n",
    "    tuner.tune(\n",
    "        nsamples,\n",
    "        measure_option=opts,\n",
    "        callbacks=[\n",
    "            autotvm.callback.progress_bar(nsamples),\n",
    "            autotvm.callback.log_to_file('test_sparse.log'),\n",
    "        ],\n",
    "    )\n",
    "autotvm.record.pick_best('test_sparse.log', 'test_sparse.best.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Name                                                       Ops                                                             Time(us)    Time(%)  Shape                  Inputs  Outputs  \n",
      "---------                                                       ---                                                             --------    -------  -----                  ------  -------  \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu        tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu        15464.8     8.613    (10, 2, 112, 112, 32)  3       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu           tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu           10226.9     5.696    (10, 56, 56, 64)       6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu1          tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu           10086.6     5.618    (10, 56, 56, 64)       6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_1         tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_1         9962.85     5.549    (10, 28, 28, 128)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu               tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu               9939.41     5.536    (10, 56, 56, 64)       5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_11        tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_1         9936.28     5.534    (10, 28, 28, 128)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_2         tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_2         9900.94     5.515    (10, 14, 14, 256)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_21        tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_2         9890.77     5.509    (10, 14, 14, 256)      6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_2             tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_2             9771.03     5.442    (10, 14, 14, 256)      5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu1              tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu               9753.11     5.432    (10, 56, 56, 64)       5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_1             tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_1             9697.75     5.401    (10, 28, 28, 128)      5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_3             tvmgen_default_fused_nn_sparse_conv2d_add_nn_relu_3             9366.94     5.217    (10, 7, 7, 512)        5       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_3         tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_3         9338.83     5.201    (10, 7, 7, 512)        6       1        \n",
      "tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_31        tvmgen_default_fused_nn_sparse_conv2d_add_add_nn_relu_3         9289.67     5.174    (10, 7, 7, 512)        6       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1      7358.91     4.099    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3      7327.92     4.081    (10, 16, 7, 7, 32)     3       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2      tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2      6802.51     3.789    (10, 4, 14, 14, 64)    3       1        \n",
      "tvmgen_default_fused_nn_max_pool2d                              tvmgen_default_fused_nn_max_pool2d                              2739.89     1.526    (10, 2, 56, 56, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_1                         tvmgen_default_fused_layout_transform_1                         2142.23     1.193    (10, 64, 56, 56)       1       1        \n",
      "tvmgen_default_fused_layout_transform_3                         tvmgen_default_fused_layout_transform_3                         1305.96     0.727    (10, 1, 56, 56, 64)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add                1025.2      0.571    (10, 2, 28, 28, 64)    3       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform          tvmgen_default_fused_layout_transform_layout_transform          896.828     0.5      (10, 56, 56, 64)       1       1        \n",
      "tvmgen_default_fused_layout_transform_4                         tvmgen_default_fused_layout_transform_4                         866.009     0.482    (10, 128, 28, 28)      1       1        \n",
      "tvmgen_default_fused_layout_transform_2                         tvmgen_default_fused_layout_transform_2                         852.749     0.475    (10, 16, 56, 56, 4)    1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2              tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2              830.98      0.463    (10, 8, 7, 7, 64)      3       1        \n",
      "tvmgen_default_fused_layout_transform                           tvmgen_default_fused_layout_transform                           764.849     0.426    (10, 3, 224, 224, 1)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1              tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1              742.399     0.413    (10, 4, 14, 14, 64)    3       1        \n",
      "tvmgen_default_fused_layout_transform_6                         tvmgen_default_fused_layout_transform_6                         496.324     0.276    (10, 1, 28, 28, 128)   1       1        \n",
      "tvmgen_default_fused_layout_transform_5                         tvmgen_default_fused_layout_transform_5                         424.856     0.237    (10, 4, 28, 28, 32)    1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_1        tvmgen_default_fused_layout_transform_layout_transform_1        407.473     0.227    (10, 28, 28, 128)      1       1        \n",
      "tvmgen_default_fused_layout_transform_7                         tvmgen_default_fused_layout_transform_7                         391.82      0.218    (10, 256, 14, 14)      1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_11       tvmgen_default_fused_layout_transform_layout_transform_1        309.094     0.172    (10, 28, 28, 128)      1       1        \n",
      "tvmgen_default_fused_layout_transform_9                         tvmgen_default_fused_layout_transform_9                         230.084     0.128    (10, 2, 14, 14, 128)   1       1        \n",
      "tvmgen_default_fused_layout_transform_8                         tvmgen_default_fused_layout_transform_8                         225.525     0.126    (10, 1, 14, 14, 256)   1       1        \n",
      "tvmgen_default_fused_nn_contrib_dense_pack_add                  tvmgen_default_fused_nn_contrib_dense_pack_add                  196.533     0.109    (10, 1000)             3       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_2        tvmgen_default_fused_layout_transform_layout_transform_2        157.076     0.087    (10, 14, 14, 256)      1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_21       tvmgen_default_fused_layout_transform_layout_transform_2        149.554     0.083    (10, 14, 14, 256)      1       1        \n",
      "tvmgen_default_fused_nn_global_avg_pool2d                       tvmgen_default_fused_nn_global_avg_pool2d                       102.294     0.057    (10, 1, 1, 512)        1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_3        tvmgen_default_fused_layout_transform_layout_transform_3        93.378      0.052    (10, 7, 7, 512)        1       1        \n",
      "tvmgen_default_fused_layout_transform_layout_transform_4        tvmgen_default_fused_layout_transform_layout_transform_4        71.902      0.04     (10, 7, 7, 512)        1       1        \n",
      "tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten  tvmgen_default_fused_layout_transform_reshape_nn_batch_flatten  3.419       0.002    (10, 512)              1       1        \n",
      "Total_time                                                      -                                                               179541.646  -        -                      -       -        \n"
     ]
    }
   ],
   "source": [
    "from tvm import autotvm\n",
    "from tvm.contrib.debugger import debug_executor\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "with autotvm.apply_history_best('test_sparse.best.log'):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build_module.build(spconst_mod3, params={}, target='llvm -mcpu=cascadelake')\n",
    "\n",
    "dev = tvm.device('llvm -mcpu=cascadelake', 0)\n",
    "data = tvm.nd.array(np.random.rand(10, 3, 224, 224).astype('float32'))\n",
    "\n",
    "graph_sparse = debug_executor.create(lib.graph_json, lib.module, dev)\n",
    "graph_sparse.set_input(data=data, **lib.params)\n",
    "\n",
    "graph_sparse.run()\n",
    "#ftimer = graph_sparse.module.time_evaluator(\"run\", dev, number=100, repeat=1)\n",
    "#ftimer().mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-build",
   "language": "python",
   "name": "tvm-build"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
