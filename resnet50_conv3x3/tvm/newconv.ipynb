{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "@register_cell_magic\n",
    "def withsave(line, cell):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('fname')\n",
    "    parser.add_argument('-f', '--force', action='store_true')\n",
    "    parser.add_argument('-a', '--append', action='store_true')\n",
    "    parser.add_argument('--subp')\n",
    "    parser.add_argument('--no-capout', action='store_false', dest='capout')\n",
    "    args = parser.parse_args(line.split())\n",
    "    if not args.fname.endswith('.py'):\n",
    "        args.fname += '.py'\n",
    "    assert not (args.force and args.append)\n",
    "    if os.path.exists(args.fname) and not (args.force or args.append):\n",
    "        raise FileExistsError(args.fname)\n",
    "    with open(args.fname, 'a' if args.append else 'w') as f:\n",
    "        f.write(cell)\n",
    "    if not args.subp:\n",
    "        get_ipython().run_cell(cell)\n",
    "    else:\n",
    "        get_ipython().user_ns[args.subp] = subprocess.run(\n",
    "            [sys.executable, args.fname],\n",
    "            capture_output=args.capout\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_convshape -f\n",
    "# resnet50\n",
    "N, H, W = 10, 512, 512\n",
    "\n",
    "convshape = [\n",
    "    (N, C, H * 32 // C, W * 32 // C)\n",
    "    for C in [64, 128, 256, 512]\n",
    "]\n",
    "\n",
    "# disable openmp\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topi.x86.conv2d_NCHWc (AutoTVM)\n",
    "\n",
    "    {N, OC, H}:para, ow =>\n",
    "        IC, kh, iic, kw:unroll, iw:unroll, oc:vec =>\n",
    "            @CC = N, OC, H, {ow, iw}/W, oc  // {IC, iic}/ic, kh, kw\n",
    "        iw, oc:vec =>\n",
    "            @CO = N, OC, H, {ow, iw}/W, oc\n",
    "\n",
    "    {N, OC, H}:para, ow =>\n",
    "        IC, kh, kw, iic, iw:unroll, oc:vec =>\n",
    "            @CC = N, OC, H, {ow, iw}/W, oc  // {IC, iic}/ic, kh, kw\n",
    "        iw, oc:vec =>\n",
    "            @CO = N, OC, H, {ow, iw}/W, oc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this out for strange error in getting device in continued tuning: https://discuss.tvm.apache.org/t/solved-autotvm-cannot-get-remote-devices-from-the-tracker/2692/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_topiconv_tune -f --subp task\n",
    "from tvm import topi, autotvm, te\n",
    "import logging\n",
    "\n",
    "from tmp_convshape import *\n",
    "Csplit = 16\n",
    "\n",
    "with open('conv2d_nchwc.dbg', 'w') as logfile:\n",
    "    logger = logging.getLogger(\"autotvm\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(logging.StreamHandler(logfile))\n",
    "    \n",
    "    for N, C, H, W in convshape:\n",
    "        Cgroups = C // Csplit\n",
    "        A = te.placeholder((N, Cgroups, H, W, Csplit), name='A')\n",
    "        B = te.placeholder((Cgroups, Cgroups, 3, 3, Csplit, Csplit), name='B')\n",
    "        task = autotvm.task.create('conv2d_NCHWc.x86',\n",
    "                                   args=(A, B, 1, 1, 1, 'NCHWc', 'NCHWc', 'float32'),\n",
    "                                   target=\"llvm -mcpu=cascadelake\")\n",
    "        \n",
    "        print(task.config_space, file=logfile)\n",
    "\n",
    "        measure_option = autotvm.measure_option(builder=\"local\",\n",
    "            runner=autotvm.LocalRunner(number=4, repeat=3, timeout=20))\n",
    "\n",
    "        tuner = autotvm.tuner.GATuner(task)\n",
    "        ntrials = 400\n",
    "        tuner.tune(\n",
    "            n_trial=ntrials,\n",
    "            measure_option=measure_option,\n",
    "            callbacks=[\n",
    "                autotvm.callback.log_to_file(\"conv2d_nchwc.log\"),\n",
    "                #autotvm.callback.progress_bar(ntrials)\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/lustre/home/acct-hpc/hpcjsl/.conda/envs/tvm-build/bin/python', 'tmp_topiconv_tune.py'], returncode=0, stdout=b'', stderr=b'')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm.topi.x86.conv2d\n",
    "autotvm.record.pick_best('conv2d_nchwc.log', 'newconv.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topi.\\*.conv2d_nhwc\n",
    "\n",
    "- topi.nn.conv2d_nhwc (w/ ansor)\n",
    "- topi.x86.schedule_conv2d_nhwc (?)\n",
    "- topi.nn.conv2d_winograd_nhwc (w/ ansor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Conv2d_3x3_gemm\n",
    "                \n",
    "    yt, xt, yo =>\n",
    "        yi, k9, ci:vec =>\n",
    "            @im2col = {yt, yo, yi}/y, {k9, ci}/k\n",
    "        xo =>\n",
    "            ko, ki:unroll, yi:unroll, xi:vec =>\n",
    "                @ccache = {yt, yo, yi}/y, {xt, xo, xi}/x  // {ko, ki}k\n",
    "            yi:unroll, xi:vec =>\n",
    "                @cout = {yt, yo, yi}/y, {xt, xo, xi}/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_myconv -f\n",
    "from tvm import autotvm, te, tir\n",
    "from functools import partial, reduce\n",
    "import tvm\n",
    "\n",
    "@autotvm.template('conv2d_3x3_gemm')\n",
    "def conv2d_3x3_gemm(N, H, W, CI, CO, dtype='float32'):\n",
    "    Y, X, K = N*H*W, CO, 9*CI\n",
    "    cfg = autotvm.get_config()\n",
    "    cfg.define_split(\"tile_y\", Y, num_outputs=3)\n",
    "    cfg.define_split(\"tile_x\", X, num_outputs=3)\n",
    "    cfg.define_split(\"tile_k\", K, num_outputs=2)\n",
    "    if cfg.is_fallback:\n",
    "        pass\n",
    "\n",
    "    data = te.placeholder((N, H, W, CI), dtype=dtype)\n",
    "    weight = te.placeholder((X, K), dtype=dtype)\n",
    "    idxsplit = lambda x,y: reduce(lambda a,b: a[:-1]+[a[-1]%b,a[-1]//b], y, [x])\n",
    "\n",
    "    @partial(te.compute, (Y, K), name='im2col')\n",
    "    def im2col(row, col):\n",
    "        jw, jh, jn = idxsplit(row, [W, H])\n",
    "        jc, kw, kh = idxsplit(col, [CI, 3])\n",
    "        ih, iw = jh + kh - 1, jw + kw - 1\n",
    "        return tir.if_then_else(\n",
    "            tir.all(0 <= ih, ih < H, 0 <= iw, iw < W),\n",
    "            data[jn, ih, iw, jc], 0)\n",
    "    \n",
    "    packw_bn = cfg[\"tile_x\"].size[-1]\n",
    "    packw = te.compute((X//packw_bn, K, packw_bn),\n",
    "        lambda xo, k, xi: weight[xo * packw_bn + xi, k],\n",
    "        name=\"packed_weight\")\n",
    "    \n",
    "    k = te.reduce_axis((0, K), name=\"k\")\n",
    "    C = te.compute((Y, X),\n",
    "        lambda y, x: te.sum(im2col[y, k] * packw[x//packw_bn, k, x%packw_bn], axis=k),\n",
    "        name=\"dense_pack\")\n",
    "    \n",
    "    s = te.create_schedule(C.op)\n",
    "    CC = s.cache_write(C, \"global\")\n",
    "    \n",
    "    y, x = s[C].op.axis\n",
    "    yt, yo, yi = cfg[\"tile_y\"].apply(s, C, y)\n",
    "    xt, xo, xi = cfg[\"tile_x\"].apply(s, C, x)\n",
    "    s[C].reorder(yt, xt, yo, xo, yi, xi)\n",
    "    #xyt = s[C].fuse(yt, xt)\n",
    "    #s[C].parallel(xyt)\n",
    "    #xyo = s[C].fuse(yo, xo)\n",
    "    s[C].unroll(yi)\n",
    "    s[C].vectorize(xi)\n",
    "\n",
    "    s[CC].compute_at(s[C], xo)\n",
    "    yi, xi = s[CC].op.axis\n",
    "    (k,) = s[CC].op.reduce_axis\n",
    "    ko, ki = cfg[\"tile_k\"].apply(s, CC, k)\n",
    "    s[CC].reorder(ko, ki, yi, xi)\n",
    "    s[CC].vectorize(xi)\n",
    "    s[CC].unroll(yi)\n",
    "    s[CC].unroll(ki)\n",
    "    \n",
    "    s[im2col].compute_at(s[C], yo)\n",
    "    yi, k = s[im2col].op.axis\n",
    "    ko, ki = s[im2col].split(k, factor=CI)\n",
    "    s[im2col].vectorize(ki)\n",
    "    #s[im2col].unroll(yi)\n",
    "\n",
    "    xo, k, xi = s[packw].op.axis\n",
    "    s[packw].reorder(xo, xi, k)\n",
    "    #s[packw].parallel(xo)\n",
    "    return s, [data, weight, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_myconv_tune -f --subp task_myconv\n",
    "from tmp_convshape import *\n",
    "from tmp_myconv import *\n",
    "import logging\n",
    "\n",
    "with open('conv2d_3x3_gemm.dbg', 'w') as logfile:\n",
    "    logger = logging.getLogger(\"autotvm\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(logging.StreamHandler(logfile))\n",
    "    \n",
    "    for N, C, H, W in convshape:\n",
    "        task = autotvm.task.create('conv2d_3x3_gemm',\n",
    "                               args=(N, H, W, C, C, 'float32'),\n",
    "                               target=\"llvm -mcpu=cascadelake\")\n",
    "        print(task.config_space, file=logfile)\n",
    "\n",
    "        measure_option = autotvm.measure_option(\n",
    "            builder=autotvm.LocalBuilder(),\n",
    "            runner=autotvm.LocalRunner(number=4, repeat=3, timeout=20))\n",
    "        tuner = autotvm.tuner.GATuner(task)\n",
    "        tuner.tune(\n",
    "            n_trial=500,\n",
    "            measure_option=measure_option,\n",
    "            callbacks=[autotvm.callback.log_to_file(\"conv2d_3x3_gemm.log\")],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/lustre/home/acct-hpc/hpcjsl/.conda/envs/tvm-build/bin/python', 'tmp_myconv_tune.py'], returncode=0, stdout=b'', stderr=b'')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_myconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotvm.record.pick_best('conv2d_3x3_gemm.log', 'newconv.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tvm import autotvm\n",
    "import numpy as np\n",
    "import tvm\n",
    "\n",
    "json_dict = {\n",
    "    \"index\": 77156, \n",
    "    \"code_hash\": None, \n",
    "    \"entity\": [\n",
    "        [\"tile_y\", \"sp\", [-1, 320, 128]], \n",
    "        [\"tile_x\", \"sp\", [-1, 8, 2]], \n",
    "        [\"tile_k\", \"sp\", [-1, 1]]\n",
    "    ]\n",
    "}\n",
    "tgtstr = \"llvm -mcpu=cascadelake\"\n",
    "ce = autotvm.task.ConfigEntity.from_json_dict(json_dict)\n",
    "with autotvm.task.ApplyConfig(ce), tvm.target.Target(tgtstr):\n",
    "    dev = tvm.device(tgtstr, 0)\n",
    "    s, params = conv2d_3x3_gemm(10, 256, 256, 64, 64, \"float32\")\n",
    "    args = [\n",
    "        tvm.nd.array(\n",
    "            np.random.rand(*[a.value for a in p.shape]).astype('float32'),\n",
    "            dev)\n",
    "        for p in params]\n",
    "    print(tvm.lower(s, params, simple_mode=True))\n",
    "    func = tvm.build(s, params, target=tgtstr, name=\"conv2d_3x3_gemm\")\n",
    "    func(*args)\n",
    "    evt = func.time_evaluator(func.entry_name, dev, number=10)\n",
    "    print(evt(*args).mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My SpConv2d_3x3_gemm\n",
    "\n",
    "    yt, xt, yo =>\n",
    "        yi, k9, ci:vec =>\n",
    "            @im2col = {yt, yo, yi}/y, {k9, ci}/k\n",
    "        xo =>\n",
    "            x1:1, ko:dyn(xr), yi:unroll, xi:vec, ki:unroll =>\n",
    "                @CC = {yt, yo, yi}/y, {xt, xo, x1}/xr, xi, ki  // ko\n",
    "            yi:unroll, xi:vec, ki:unroll =>\n",
    "                @C = {yt, yo, yi}/y, {xt, xo, xi}/x  // ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_myspconv -f\n",
    "from tvm import autotvm, te, tir\n",
    "from functools import partial, reduce\n",
    "\n",
    "@autotvm.template('spconv2d_3x3_gemm')\n",
    "def spconv2d_3x3_gemm(N, H, W, CI, CO, nElems, bsrR, bsrC, dtype='float32'):\n",
    "    Y, X, K = N*H*W, CO, 9*CI\n",
    "    cfg = autotvm.get_config()\n",
    "    cfg.define_split(\"tile_y\", Y, num_outputs=3)\n",
    "    cfg.define_split(\"tile_x\", X // bsrR, num_outputs=2)\n",
    "    cfg.add_flop(Y * (nElems * bsrC * bsrR * 2 - X))\n",
    "    #cfg.define_split(\"tile_k\", K, num_outputs=2)\n",
    "    if cfg.is_fallback:\n",
    "        cfg['tile_y'] = autotvm.task.space.SplitEntity([-1, 160, 8])\n",
    "        cfg['tile_x'] = autotvm.task.space.SplitEntity([-1, 4])\n",
    "    \n",
    "    Data = te.placeholder((N, H, W, CI), dtype=dtype, name='Data')\n",
    "    Wdat = te.placeholder((nElems, bsrR, bsrC), name='Wdat')\n",
    "    Wind = te.placeholder((nElems,), dtype='int', name='Wind')\n",
    "    Wptr = te.placeholder((X // bsrR + 1,), dtype='int', name='Wptr')\n",
    "    idxsplit = lambda x,y: reduce(lambda a,b: a[:-1]+[a[-1]%b,a[-1]//b], y, [x])\n",
    "\n",
    "    @partial(te.compute, (Y, K), name='Im2Col')\n",
    "    def Im2Col(row, col):\n",
    "        jw, jh, jn = idxsplit(row, [W, H])\n",
    "        jc, kw, kh = idxsplit(col, [CI, 3])\n",
    "        ih, iw = jh + kh - 1, jw + kw - 1\n",
    "        return tir.if_then_else(\n",
    "            tir.all(0 <= ih, ih < H, 0 <= iw, iw < W),\n",
    "            Data[jn, ih, iw, jc], 0)\n",
    "    \n",
    "    @partial(te.compute, (Y, X // bsrR, bsrR, bsrC), name='CC')\n",
    "    def CC(drow, wrow, brow, bcol):\n",
    "        row_start, row_end = Wptr[wrow], Wptr[wrow+1]\n",
    "        elem_idx = te.reduce_axis((0, row_end - row_start), name='elem_idx')\n",
    "        elem = row_start + elem_idx\n",
    "        return te.sum(Im2Col[drow, Wind[elem]*bsrC + bcol] * Wdat[elem, brow, bcol], axis=elem_idx)\n",
    "\n",
    "    k = te.reduce_axis((0, bsrC), name='k')\n",
    "    C = te.compute((Y, X), lambda y, x: te.sum(CC[y, x // bsrR, x % bsrR, k], axis=k), name='C')\n",
    "    \n",
    "    s = te.create_schedule(C.op)\n",
    "    y, x = s[C].op.axis\n",
    "    yt, yo, yi = cfg['tile_y'].apply(s, C, y)\n",
    "    xo, xi = s[C].split(x, factor=bsrR)\n",
    "    xt, xo = cfg['tile_x'].apply(s, C, xo)\n",
    "    (k,) = s[C].op.reduce_axis\n",
    "    s[C].reorder(yt, xt, yo, xo, yi, xi, k)\n",
    "    s[C].unroll(k)\n",
    "    s[C].vectorize(xi)\n",
    "    s[C].unroll(yi)\n",
    "\n",
    "    s[CC].compute_at(s[C], xo)\n",
    "    yi, xi, r, c = s[CC].op.axis\n",
    "    (k,) = s[CC].op.reduce_axis\n",
    "    s[CC].reorder(xi, k, yi, r, c)\n",
    "    s[CC].unroll(c)\n",
    "    s[CC].vectorize(r)\n",
    "    s[CC].unroll(yi)\n",
    "    \n",
    "    s[Im2Col].compute_at(s[C], yo)\n",
    "    yi, k = s[Im2Col].op.axis\n",
    "    ko, ki = s[Im2Col].split(k, factor=CI)\n",
    "    s[Im2Col].vectorize(ki)\n",
    "    #s[Im2Col].unroll(yi)\n",
    "    return s, [Data, Wdat, Wind, Wptr, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [autotvm ref_input removed](https://github.com/apache/tvm/commit/b5a7de879e67aca80aa25bf9ea9c46315dccb026#diff-f8cbe8a70063c3692732fa42db6f11779f92eb2afeb5576b68b7ede8064a8222L596)\n",
    "- [ansor task_inputs](https://tvm.apache.org/docs/tutorials/auto_scheduler/tune_sparse_x86.html#create-the-search-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_mysputils -f\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.rpc import RPCSession\n",
    "\n",
    "def make_bsr_sparse(dense, sprate, blocksize):\n",
    "    bsrdata = scipy.sparse.bsr_matrix(dense, blocksize=blocksize)\n",
    "    # find partition value\n",
    "    summed = bsrdata.data.sum((1, 2))\n",
    "    idx = int(sprate * len(summed) + 0.5)\n",
    "    val = np.partition(summed, idx)[idx]\n",
    "    # filter the data\n",
    "    data, indices, indptr, bsrWid = [], [], [], bsrdata.indptr[1]\n",
    "    for idx, (block, indval) in enumerate(zip(bsrdata.data, bsrdata.indices)):\n",
    "        if idx % bsrWid == 0:\n",
    "            indptr.append(len(data))\n",
    "        if block.sum() >= val:\n",
    "            data.append(block)\n",
    "            indices.append(indval)\n",
    "    indptr.append(len(data))\n",
    "    # convert format\n",
    "    bsrdata2 = tuple([np.array(i) for i in [data, indices, indptr]])\n",
    "    return scipy.sparse.bsr_matrix(bsrdata2, shape=dense.shape)\n",
    "\n",
    "\n",
    "def unpack_bsr(bsrdata):\n",
    "    return bsrdata.data, bsrdata.indices, bsrdata.indptr\n",
    "\n",
    "\n",
    "def hook_method(obj, attr):\n",
    "    def real_decorator(func):\n",
    "        orig = getattr(obj, attr)\n",
    "        setattr(obj, attr, func)\n",
    "        func.orig = orig\n",
    "        func.revert = lambda: setattr(obj, attr, orig)\n",
    "        return func\n",
    "    return real_decorator\n",
    "\n",
    "\n",
    "class NonRandomFill:\n",
    "    srclst_ = []\n",
    "    \n",
    "    @classmethod\n",
    "    def set_srclst(cls, srclst):\n",
    "        cls.srclst_ = [tvm.nd.array(it) for it in srclst]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.srclst = iter(self.srclst_)\n",
    "    \n",
    "    def __call__(self, tgt):\n",
    "        src = next(self.srclst)\n",
    "        tgt.copyfrom(src)\n",
    "\n",
    "\n",
    "@hook_method(RPCSession, 'get_function')\n",
    "def new_get_function(self, fname):\n",
    "    if fname == 'tvm.contrib.random.random_fill':\n",
    "        return NonRandomFill()\n",
    "    else:\n",
    "        return new_get_function.orig(self, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%withsave tmp_myspconv_tune -f --subp task_spconv\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from tmp_convshape import *\n",
    "from tmp_myspconv import *\n",
    "from tmp_mysputils import *\n",
    "\n",
    "with open('spconv2d_3x3_gemm.dbg', 'w') as logfile:\n",
    "    logger = logging.getLogger(\"autotvm\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(logging.StreamHandler(logfile))\n",
    "\n",
    "    for N, C, H, W in convshape:\n",
    "        for sprate in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "            nhwc_data = np.random.randint(0, 256, (N, H, W, C)).astype('float32')\n",
    "            weight_ohwi = np.random.rand(C, 3*3*C).astype('float32')\n",
    "            spweight_ohwi = make_bsr_sparse(weight_ohwi, sprate, (16, 1))\n",
    "            ret = np.zeros((N*H*W, C), dtype='float32')\n",
    "\n",
    "            args = (N, H, W, C, C, *spweight_ohwi.data.shape, 'float32')\n",
    "            task = autotvm.task.create('spconv2d_3x3_gemm', args=args, target=\"llvm -mcpu=cascadelake\")\n",
    "            print(task.config_space, file=logfile)\n",
    "\n",
    "            runner = autotvm.LocalRunner(number=4, repeat=3, timeout=20)\n",
    "            NonRandomFill.set_srclst([nhwc_data, *unpack_bsr(spweight_ohwi), ret])\n",
    "            measure_option = autotvm.measure_option(builder=autotvm.LocalBuilder(), runner=runner)\n",
    "            tuner = autotvm.tuner.GATuner(task)\n",
    "            tuner.tune(\n",
    "                n_trial=500,\n",
    "                measure_option=measure_option,\n",
    "                callbacks=[autotvm.callback.log_to_file(\"spconv2d_3x3_gemm.log\")],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_spconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ansor\n",
    "\n",
    "> [Ansor sparse_dense tutorial](https://tvm.apache.org/docs/tutorials/auto_scheduler/tune_sparse_x86.html)\n",
    "\n",
    "sparse_dense_sp_rhs_bsrmm\n",
    "```\n",
    "BM:= m, nblk, br, R{elem:dyn(nblk), bc:1}\n",
    "RS:= m, n{nblk, br}\n",
    "BM,RS> {m, nblk}:para\n",
    "  BM> {br, elem:dyn(nblk), bc:1}:reorder\n",
    "    BM> elem:dyn(nblk), br:vec, bc:1\n",
    "  RS> br:vec\n",
    "```\n",
    "\n",
    "1. task_input：提供下标输入，避免按随机输入搜索时暴毙\n",
    "2. custom sketch：提供一个手写的init search policy\n",
    "    - 作用于什么op\n",
    "    - 如何进行init sketch，需要用ansor专门的一套LoopState API；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The new TVM Schedule Representation\n",
    "\n",
    "## TVM基本原语\n",
    "\n",
    "TVM提供了一套在算子定义之外、对算子的循环层次进行编辑的操作指令。比较主要的包括：\n",
    "\n",
    "- compute dag变换\n",
    "    - `cache_*`和`rfactor`：一些会增加新stage的高层变换\n",
    "- 轴重组\n",
    "    - `split`和`reorder`：进行`tile`这样的访存顺序调整的原语\n",
    "    - `compute_*`：一组用来对多个stage进行重组、嵌套的原语，会引起隐式的`split`\n",
    "    - `fuse`：组合若干循环维度，本身没有任何用，作为其他变换的前置条件\n",
    "- 单一轴\n",
    "    - `vectorize`，`unroll`和`parallel`：指定某一循环维度具体实现方式的原语\n",
    "    \n",
    "实际上令人耗费精力的主要是前两类schedule，第三类schedule基本可以通过启发式的方法做出简单的少数几种策略。\n",
    "\n",
    "另有gpu schedule原语 https://tvm.apache.org/docs/tutorials/optimize/opt_conv_cuda.html\n",
    "\n",
    "命令式语言，可以灵活的进行版本分支；缺乏直观性、持久性。\n",
    "\n",
    "## 现有TVM DSL/可视化\n",
    "\n",
    "[Tensor Expression Debug Display](https://tvm.apache.org/docs/tutorials/language/tedd.html)或[Operational Model (TVMConf'19)](https://sampl.cs.washington.edu/tvmconf/slides/2019/E03-Yuan-Lin-Yongfeng-Gu.pdf)，包括三张图：\n",
    "\n",
    "- ComputeDAG: Stage间的依赖\n",
    "- IterVarRelation: IterVar间的演化\n",
    "- ScheduleTree: 实现策略，包括Stage融合、LeafIterVar行为\n",
    "\n",
    "评价：\n",
    "\n",
    "- 把IterVar变换和Schedule变化进行了分离；前者具有很强的操作性特征，而后者基本是声明式的；\n",
    "    - 如果只进行一轮`split`-`reorder`-`fuse`，那么IterVar变换也是声明式的；然而没有保证；\n",
    "- 是否需要一个文字版的schedule tree？增加可写性，改善直观性；一种更简单的IR；\n",
    "- IterVarRel的多版本需要改进的树形表示；\n",
    "- Compute_\\*依然是抽象问题；源于[InferBound](https://tvm.apache.org/docs/dev/inferbound.html)机制，即TVM通过infer机制尽可能减少计算的域的大小；但这套机制很难处理复杂的fuse和split的组合。\n",
    "\n",
    "事实上这种面向轴的schedule语言可以产出非常多没什么道理的schedule；DSL的使命应该是利用domain knowledge，减少开发过程的自由度。\n",
    "\n",
    "- [Fireiron (TVMConf'19)](https://tvmconf.org/slides/2019/E04-Vinod-Grover.pdf)是一种面向GPU的DSL。除了常见的decomp原语（split，tile）之外，还对结构化的bind、cache进行了支持；\n",
    "- Ansor\n",
    "\n",
    "    - 论文中Ansor拒绝了线性决策式的schedule构造（schedule原语的线性累加），自我标榜为一种“层次化”方法。\n",
    "    - high-level structure不仅不确定参数，衍生规则也相对高层：inline、tiling、cachewrite、rfact。IterVar的重组完全由多层tile的RS表达式确定。\n",
    "    - ansor中有layout free tensor的概念：对于weight数据，可以直接一次性改写为匹配搜索结果的数据格式。即在合适的rule设计下，ansor具有数据格式搜索能力。\n",
    "    - ansor论文在single op部分的eval中提到了一些ansor具有优势的例子，包括rfact的广泛使用、在tile和inline上的灵活性。tile的灵活性体现在层次数量而非排列；当层次数量足够多时，即使排列固定，也能有非常丰富的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-build",
   "language": "python",
   "name": "tvm-build"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
