{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Automatic calling is: Smart\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "%autocall\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te, tir\n",
    "# llc --version\n",
    "target = \"llvm -mcpu=cascadelake\"  # cascadelake znver2\n",
    "ctx = tvm.context(target, 0)\n",
    "vec = 8\n",
    "\n",
    "\n",
    "def idxsplit(idx, dim, *dim2):\n",
    "    if dim2:\n",
    "        idx, *lower = idxsplit(idx, *dim2)\n",
    "    else:\n",
    "        lower = []\n",
    "    return (idx // dim, idx % dim, *lower)\n",
    "\n",
    "\n",
    "class TVMRunner:\n",
    "    def __init__(self, name, params):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "    \n",
    "    def lower(self):\n",
    "        return tvm.lower(*self.params, simple_mode=True)\n",
    "    \n",
    "    def _wrap_args(self, args):\n",
    "        def _wrap_single(item, item2):\n",
    "            if isinstance(item, np.ndarray):\n",
    "                return tvm.nd.array(item, ctx)\n",
    "            elif isinstance(item, type):\n",
    "                shape = [it.value for it in item2.shape]\n",
    "                return tvm.nd.array(np.zeros(shape, dtype=item), ctx)\n",
    "            else:\n",
    "                raise Exception('unknown arg', item)\n",
    "\n",
    "        _, args2 = self.params\n",
    "        realargs = [_wrap_single(it, it2) for it, it2 in zip(args, args2)]\n",
    "        return realargs\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        func = tvm.build(*self.params, target=target, name=self.name)\n",
    "        realargs = self._wrap_args(args)\n",
    "        func(*realargs)\n",
    "        return realargs\n",
    "    \n",
    "    def time_eval(self, *args, number=10):\n",
    "        func = tvm.build(*self.params, target=target, name=self.name)\n",
    "        realargs = self._wrap_args(args)\n",
    "        evaluator = func.time_evaluator(func.entry_name, ctx, number=number)\n",
    "        print(evaluator(*realargs).mean)\n",
    "        return realargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_oihw = np.random.rand(64, 64, 3, 3).astype('float32')\n",
    "weight_ohwi = np.moveaxis(weight_oihw, 1, -1)\n",
    "weight_ohwi_flat = weight_ohwi.reshape((weight_ohwi.shape[0], -1))\n",
    "nchw_data = np.random.randint(0, 256, (10, 64, 256, 256)).astype('float32')\n",
    "nhwc_data = np.moveaxis(nchw_data, 1, -1)\n",
    "\n",
    "del weight_oihw\n",
    "del weight_ohwi\n",
    "del nchw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bsr_sparse(dense, sprate, blocksize):\n",
    "    bsrdata = scipy.sparse.bsr_matrix(dense, blocksize=blocksize)\n",
    "    # find partition value\n",
    "    summed = bsrdata.data.sum((1, 2))\n",
    "    idx = int(sprate * len(summed) + 0.5)\n",
    "    val = np.partition(summed, idx)[idx]\n",
    "    # filter the data\n",
    "    data, indices, indptr, bsrWid = [], [], [], bsrdata.indptr[1]\n",
    "    for idx, (block, indval) in enumerate(zip(bsrdata.data, bsrdata.indices)):\n",
    "        if idx % bsrWid == 0:\n",
    "            indptr.append(len(data))\n",
    "        if block.sum() >= val:\n",
    "            data.append(block)\n",
    "            indices.append(indval)\n",
    "    indptr.append(len(data))\n",
    "    # convert format\n",
    "    bsrdata2 = tuple([np.array(i) for i in [data, indices, indptr]])\n",
    "    return scipy.sparse.bsr_matrix(bsrdata2, shape=dense.shape)\n",
    "\n",
    "\n",
    "def unpack_bsr(bsrdata):\n",
    "    return bsrdata.data, bsrdata.indices, bsrdata.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1724701032\n"
     ]
    }
   ],
   "source": [
    "def create_nhwc_im2col(data):\n",
    "    N, H, W, C = inshape = data.shape\n",
    "    A = te.placeholder(inshape, name='A')\n",
    "\n",
    "    def im2col_kernel(row, col):\n",
    "        jn, jh, jw = idxsplit(row, H, W)\n",
    "        kh, kw, jc = idxsplit(col, 3, C)\n",
    "        ih, iw = jh + kh - 1, jw + kw - 1\n",
    "        return tir.if_then_else(\n",
    "            tir.all(0 <= ih, ih < H, 0 <= iw, iw < W),\n",
    "            A[jn, ih, iw, jc], 0)\n",
    "\n",
    "    outshape = (N*H*W, 9*C)\n",
    "    B = te.compute(outshape, im2col_kernel, name='B')\n",
    "\n",
    "    def im2col_schedule(CC):\n",
    "        s = te.create_schedule(CC.op)\n",
    "        _, coldim = s[CC].op.axis\n",
    "        _, chandim = s[CC].split(coldim, factor=C)\n",
    "        s[CC].vectorize(chandim)\n",
    "        return s\n",
    "    \n",
    "    s = im2col_schedule(B)\n",
    "    return s, [A, B]\n",
    "\n",
    "\n",
    "tr = TVMRunner('im2col', create_nhwc_im2col(nhwc_data))\n",
    "_, ret = tr.time_eval(nhwc_data, np.float32)\n",
    "nhwkkc_data = ret.asnumpy()\n",
    "kkcnhw_data = nhwkkc_data.T\n",
    "del ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Trans GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit nhwkkc_data @ weight_ohwi_flat.T\n",
    "#%timeit np.tensordot(nhwkkc_data, weight_ohwi_flat, [[1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [655360, 64], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [655360, 576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [64, 576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  attr [CC: Pointer(float32)] \"storage_scope\" = \"global\";\n",
      "  allocate(CC, float32, [1024]);\n",
      "  for (m.outer: int32, 0, 40960) {\n",
      "    for (n.outer: int32, 0, 8) {\n",
      "      for (m.init: int32, 0, 16) {\n",
      "        for (n.init: int32, 0, 8) {\n",
      "          CC[ramp(((m.init*64) + (n.init*8)), 1, 8)] = broadcast(0f32, 8)\n",
      "        }\n",
      "      }\n",
      "      for (kk.outer: int32, 0, 18) {\n",
      "        for (m: int32, 0, 16) {\n",
      "          for (n: int32, 0, 8) {\n",
      "            CC[ramp(((m*64) + (n*8)), 1, 8)] = ((float32x8*)CC[ramp(((m*64) + (n*8)), 1, 8)] + ((float32x8*)A_2[ramp((((m.outer*9216) + (m*576)) + (kk.outer*32)), 1, 8)]*(float32x8*)B_2[ramp((((n.outer*4608) + (n*576)) + (kk.outer*32)), 1, 8)]))\n",
      "            CC[ramp(((m*64) + (n*8)), 1, 8)] = ((float32x8*)CC[ramp(((m*64) + (n*8)), 1, 8)] + ((float32x8*)A_2[ramp(((((m.outer*9216) + (m*576)) + (kk.outer*32)) + 8), 1, 8)]*(float32x8*)B_2[ramp(((((n.outer*4608) + (n*576)) + (kk.outer*32)) + 8), 1, 8)]))\n",
      "            CC[ramp(((m*64) + (n*8)), 1, 8)] = ((float32x8*)CC[ramp(((m*64) + (n*8)), 1, 8)] + ((float32x8*)A_2[ramp(((((m.outer*9216) + (m*576)) + (kk.outer*32)) + 16), 1, 8)]*(float32x8*)B_2[ramp(((((n.outer*4608) + (n*576)) + (kk.outer*32)) + 16), 1, 8)]))\n",
      "            CC[ramp(((m*64) + (n*8)), 1, 8)] = ((float32x8*)CC[ramp(((m*64) + (n*8)), 1, 8)] + ((float32x8*)A_2[ramp(((((m.outer*9216) + (m*576)) + (kk.outer*32)) + 24), 1, 8)]*(float32x8*)B_2[ramp(((((n.outer*4608) + (n*576)) + (kk.outer*32)) + 24), 1, 8)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      for (m.inner: int32, 0, 16) {\n",
      "        for (n.inner: int32, 0, 8) {\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = 0f32\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[((m.inner*64) + (n.inner*8))])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 1)])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 2)])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 3)])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 4)])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 5)])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 6)])\n",
      "          C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] = ((float32*)C_2[((((m.outer*1024) + (m.inner*64)) + (n.outer*8)) + n.inner)] + (float32*)CC[(((m.inner*64) + (n.inner*8)) + 7)])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "0.7636745701\n"
     ]
    }
   ],
   "source": [
    "def create_dense_trans_gemm(data, weight):\n",
    "    M, K, N, _ = *data.shape, *weight.shape\n",
    "    A = te.placeholder((M, K), name='A')\n",
    "    B = te.placeholder((N, K), name='B')\n",
    "    kk = te.reduce_axis((0, K // vec), name='kk')\n",
    "    CC = te.compute((M, N, vec),\n",
    "                    lambda m, n, v: te.sum(A[m, kk*vec + v] * B[n, kk*vec + v], axis=kk), name='CC')\n",
    "    kv = te.reduce_axis((0, vec), name='kv')\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(CC[m, n, kv], axis=kv), name='C')\n",
    "\n",
    "    def create_dense_schedule(C, CC):\n",
    "        s = te.create_schedule(C.op)\n",
    "        m, n = s[C].op.axis\n",
    "        (kv,) = s[C].op.reduce_axis\n",
    "        mo, no, mi, ni = s[C].tile(m, n, 16, 8)\n",
    "        s[C].unroll(kv)\n",
    "\n",
    "        s[CC].compute_at(s[C], no)\n",
    "        mi, ni, v = s[CC].op.axis\n",
    "        (kk,) = s[CC].op.reduce_axis\n",
    "        ko, ki = s[CC].split(kk, factor=4)\n",
    "        s[CC].reorder(ko, mi, ni, ki, v)\n",
    "        s[CC].vectorize(v)\n",
    "        s[CC].unroll(ki)\n",
    "        return s\n",
    "\n",
    "    s = create_dense_schedule(C, CC)\n",
    "    return s, [A, B, C]\n",
    "\n",
    "\n",
    "tr = TVMRunner('dense_trans_gemm', create_dense_trans_gemm(nhwkkc_data, weight_ohwi_flat))\n",
    "print(tr.lower())\n",
    "tr.time_eval(nhwkkc_data, weight_ohwi_flat, np.float32)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse NonTrans GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsr_2x1 = make_bsr_sparse(weight_ohwi_flat, 0.5, (4, 1))\n",
    "#%timeit bsr_2x1 * kkcnhw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(Wdat_1: handle, Wind_1: handle, Wptr_1: handle, Data_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {Data: Buffer(Data_2: Pointer(float32), float32, [576, 655360], []),\n",
      "             Wdat: Buffer(Wdat_2: Pointer(float32), float32, [4608, 4, 1], []),\n",
      "             Wind: Buffer(Wind_2: Pointer(int32), int32, [4608], []),\n",
      "             Wptr: Buffer(Wptr_2: Pointer(int32), int32, [17], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [64, 655360], [])}\n",
      "  buffer_map = {Wptr_1: Wptr, C_1: C, Wdat_1: Wdat, Data_1: Data, Wind_1: Wind} {\n",
      "  attr [CC: Pointer(float32)] \"storage_scope\" = \"global\";\n",
      "  allocate(CC, float32x16, [32]);\n",
      "  for (n.outer: int32, 0, 5120) {\n",
      "    for (m.outer: int32, 0, 16) {\n",
      "      for (dcol.outer.init: int32, 0, 8) {\n",
      "        CC[ramp((dcol.outer.init*16), 1, 16)] = broadcast(0f32, 16)\n",
      "        CC[ramp(((dcol.outer.init*16) + 128), 1, 16)] = broadcast(0f32, 16)\n",
      "        CC[ramp(((dcol.outer.init*16) + 256), 1, 16)] = broadcast(0f32, 16)\n",
      "        CC[ramp(((dcol.outer.init*16) + 384), 1, 16)] = broadcast(0f32, 16)\n",
      "      }\n",
      "      for (elem_idx: int32, 0, ((int32*)Wptr_2[(m.outer + 1)] - (int32*)Wptr_2[m.outer])) {\n",
      "        for (dcol.outer: int32, 0, 8) {\n",
      "          CC[ramp((dcol.outer*16), 1, 16)] = ((float32x16*)CC[ramp((dcol.outer*16), 1, 16)] + ((float32x16*)Data_2[ramp(((((int32*)Wind_2[((int32*)Wptr_2[m.outer] + elem_idx)]*655360) + (n.outer*128)) + (dcol.outer*16)), 1, 16)]*broadcast((float32*)Wdat_2[(((int32*)Wptr_2[m.outer]*4) + (elem_idx*4))], 16)))\n",
      "          CC[ramp(((dcol.outer*16) + 128), 1, 16)] = ((float32x16*)CC[ramp(((dcol.outer*16) + 128), 1, 16)] + ((float32x16*)Data_2[ramp(((((int32*)Wind_2[((int32*)Wptr_2[m.outer] + elem_idx)]*655360) + (n.outer*128)) + (dcol.outer*16)), 1, 16)]*broadcast((float32*)Wdat_2[((((int32*)Wptr_2[m.outer]*4) + (elem_idx*4)) + 1)], 16)))\n",
      "          CC[ramp(((dcol.outer*16) + 256), 1, 16)] = ((float32x16*)CC[ramp(((dcol.outer*16) + 256), 1, 16)] + ((float32x16*)Data_2[ramp(((((int32*)Wind_2[((int32*)Wptr_2[m.outer] + elem_idx)]*655360) + (n.outer*128)) + (dcol.outer*16)), 1, 16)]*broadcast((float32*)Wdat_2[((((int32*)Wptr_2[m.outer]*4) + (elem_idx*4)) + 2)], 16)))\n",
      "          CC[ramp(((dcol.outer*16) + 384), 1, 16)] = ((float32x16*)CC[ramp(((dcol.outer*16) + 384), 1, 16)] + ((float32x16*)Data_2[ramp(((((int32*)Wind_2[((int32*)Wptr_2[m.outer] + elem_idx)]*655360) + (n.outer*128)) + (dcol.outer*16)), 1, 16)]*broadcast((float32*)Wdat_2[((((int32*)Wptr_2[m.outer]*4) + (elem_idx*4)) + 3)], 16)))\n",
      "        }\n",
      "      }\n",
      "      for (n.inner.outer: int32, 0, 8) {\n",
      "        C_2[ramp((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)), 1, 16)] = broadcast(0f32, 16)\n",
      "        C_2[ramp((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)), 1, 16)] = ((float32x16*)C_2[ramp((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)), 1, 16)] + (float32x16*)CC[ramp((n.inner.outer*16), 1, 16)])\n",
      "        C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 655360), 1, 16)] = broadcast(0f32, 16)\n",
      "        C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 655360), 1, 16)] = ((float32x16*)C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 655360), 1, 16)] + (float32x16*)CC[ramp(((n.inner.outer*16) + 128), 1, 16)])\n",
      "        C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 1310720), 1, 16)] = broadcast(0f32, 16)\n",
      "        C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 1310720), 1, 16)] = ((float32x16*)C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 1310720), 1, 16)] + (float32x16*)CC[ramp(((n.inner.outer*16) + 256), 1, 16)])\n",
      "        C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 1966080), 1, 16)] = broadcast(0f32, 16)\n",
      "        C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 1966080), 1, 16)] = ((float32x16*)C_2[ramp(((((m.outer*2621440) + (n.outer*128)) + (n.inner.outer*16)) + 1966080), 1, 16)] + (float32x16*)CC[ramp(((n.inner.outer*16) + 384), 1, 16)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "1.7704064682999998\n"
     ]
    }
   ],
   "source": [
    "def create_nontrans_gemm(bsr, dense):\n",
    "    M, K, _, N, bsrR, bsrC = *bsr.shape, *dense.shape, *bsr.blocksize\n",
    "    bsrdata, bsrindices, bsrindptr = unpack_bsr(bsr)\n",
    "    Wdat = te.placeholder(bsrdata.shape, name='Wdat')\n",
    "    Wind = te.placeholder(bsrindices.shape, dtype='int', name='Wind')\n",
    "    Wptr = te.placeholder(bsrindptr.shape, dtype='int', name='Wptr')\n",
    "    Data = te.placeholder(dense.shape, name='Data')\n",
    "    \n",
    "    def bsr_gemm_kernel(wrow, brow, dcol, bcol):\n",
    "        row_start, row_end = Wptr[wrow], Wptr[wrow+1]\n",
    "        elem_idx = te.reduce_axis((0, row_end - row_start), name='elem_idx')\n",
    "        elem = row_start + elem_idx\n",
    "        return te.sum(Data[Wind[elem]*bsrC + bcol, dcol] * Wdat[elem, brow, bcol], axis=elem_idx)\n",
    "\n",
    "    CC = te.compute((M // bsrR, bsrR, N, bsrC), bsr_gemm_kernel, name='CC')\n",
    "    k = te.reduce_axis((0, bsrC), name='k')\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(CC[m // bsrR, m % bsrR, n, k], axis=k), name='C')\n",
    "    \n",
    "    def create_bsr_gemm_schedule(C, CC):\n",
    "        s = te.create_schedule(C.op)\n",
    "        md, nd = s[C].op.axis\n",
    "        kd = s[C].op.reduce_axis[0]\n",
    "        md, nd1, rd, nd2 = s[C].tile(md, nd, bsrR, 16*vec)\n",
    "        s[C].reorder(nd1, md, nd2, rd, kd)\n",
    "        s[C].unroll(kd)\n",
    "        s[C].unroll(rd)\n",
    "        nd2a, nd2b = s[C].split(nd2, nparts=8)\n",
    "        s[C].vectorize(nd2b)\n",
    "        \n",
    "        s[CC].compute_at(s[C], md)\n",
    "        md, rd, nd2, cd = s[CC].op.axis\n",
    "        (ed,) = s[CC].op.reduce_axis\n",
    "        s[CC].reorder(md, ed, nd2, rd, cd)\n",
    "        s[CC].unroll(cd)\n",
    "        s[CC].unroll(rd)\n",
    "        nd2a, nd2b = s[CC].split(nd2, nparts=8)\n",
    "        s[CC].vectorize(nd2b)\n",
    "        return s\n",
    "    \n",
    "    s = create_bsr_gemm_schedule(C, CC)\n",
    "    return s, [Wdat, Wind, Wptr, Data, C]\n",
    "\n",
    "\n",
    "tr = TVMRunner('bsr_nontrans_gemm', create_nontrans_gemm(bsr_2x1, kkcnhw_data))\n",
    "print(tr.lower())\n",
    "tr.time_eval(*unpack_bsr(bsr_2x1), kkcnhw_data, np.float32)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Trans GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsr_1x2 = make_bsr_sparse(weight_ohwi_flat, 0.5, (1, 4))\n",
    "#%timeit nhwkkc_data * bsr_1x2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(Data_1: handle, Wdat_1: handle, Wind_1: handle, Wptr_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {Wind: Buffer(Wind_2: Pointer(int32), int32, [4608], []),\n",
      "             Wptr: Buffer(Wptr_2: Pointer(int32), int32, [65], []),\n",
      "             Data: Buffer(Data_2: Pointer(float32), float32, [655360, 576], []),\n",
      "             Wdat: Buffer(Wdat_2: Pointer(float32), float32, [4608, 1, 4], [])}\n",
      "  buffer_map = {Data_1: Data, Wdat_1: Wdat, Wind_1: Wind, Wptr_1: Wptr} {\n",
      "  attr [C: Pointer(float32)] \"storage_scope\" = \"global\";\n",
      "  allocate(C, float32x8, [5242880]);\n",
      "  attr [CC: Pointer(float32)] \"storage_scope\" = \"global\";\n",
      "  allocate(CC, float32, [32]);\n",
      "  for (m.outer: int32, 0, 81920) {\n",
      "    for (n.outer: int32, 0, 64) {\n",
      "      CC[ramp(0, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(4, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(8, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(12, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(16, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(20, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(24, 1, 4)] = broadcast(0f32, 4)\n",
      "      CC[ramp(28, 1, 4)] = broadcast(0f32, 4)\n",
      "      for (elem_idx: int32, 0, ((int32*)Wptr_2[(n.outer + 1)] - (int32*)Wptr_2[n.outer])) {\n",
      "        CC[ramp(0, 1, 4)] = ((float32x4*)CC[ramp(0, 1, 4)] + ((float32x4*)Data_2[ramp(((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(4, 1, 4)] = ((float32x4*)CC[ramp(4, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 576), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(8, 1, 4)] = ((float32x4*)CC[ramp(8, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 1152), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(12, 1, 4)] = ((float32x4*)CC[ramp(12, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 1728), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(16, 1, 4)] = ((float32x4*)CC[ramp(16, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 2304), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(20, 1, 4)] = ((float32x4*)CC[ramp(20, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 2880), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(24, 1, 4)] = ((float32x4*)CC[ramp(24, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 3456), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "        CC[ramp(28, 1, 4)] = ((float32x4*)CC[ramp(28, 1, 4)] + ((float32x4*)Data_2[ramp((((m.outer*4608) + ((int32*)Wind_2[((int32*)Wptr_2[n.outer] + elem_idx)]*4)) + 4032), 1, 4)]*(float32x4*)Wdat_2[ramp((((int32*)Wptr_2[n.outer]*4) + (elem_idx*4)), 1, 4)]))\n",
      "      }\n",
      "      C[ramp(((m.outer*512) + n.outer), 64, 8)] = broadcast(0f32, 8)\n",
      "      C[ramp(((m.outer*512) + n.outer), 64, 8)] = ((float32x8*)C[ramp(((m.outer*512) + n.outer), 64, 8)] + (float32x8*)CC[ramp(0, 4, 8)])\n",
      "      C[ramp(((m.outer*512) + n.outer), 64, 8)] = ((float32x8*)C[ramp(((m.outer*512) + n.outer), 64, 8)] + (float32x8*)CC[ramp(1, 4, 8)])\n",
      "      C[ramp(((m.outer*512) + n.outer), 64, 8)] = ((float32x8*)C[ramp(((m.outer*512) + n.outer), 64, 8)] + (float32x8*)CC[ramp(2, 4, 8)])\n",
      "      C[ramp(((m.outer*512) + n.outer), 64, 8)] = ((float32x8*)C[ramp(((m.outer*512) + n.outer), 64, 8)] + (float32x8*)CC[ramp(3, 4, 8)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "0.8443843999\n"
     ]
    }
   ],
   "source": [
    "def create_trans_gemm(dense, bsr):\n",
    "    M, K, N, _, bsrR, bsrC = *dense.shape, *bsr.shape, *bsr.blocksize\n",
    "    bsrdata, bsrindices, bsrindptr = unpack_bsr(bsr)\n",
    "    Data = te.placeholder(dense.shape, name='Data')\n",
    "    Wdat = te.placeholder(bsrdata.shape, name='Wdat')\n",
    "    Wind = te.placeholder(bsrindices.shape, dtype='int', name='Wind')\n",
    "    Wptr = te.placeholder(bsrindptr.shape, dtype='int', name='Wptr')\n",
    "    \n",
    "    def bsr_gemm_kernel(drow, wrow, brow, bcol):\n",
    "        row_start, row_end = Wptr[wrow], Wptr[wrow+1]\n",
    "        elem_idx = te.reduce_axis((0, row_end - row_start), name='elem_idx')\n",
    "        elem = row_start + elem_idx\n",
    "        return te.sum(Data[drow, Wind[elem]*bsrC + bcol] * Wdat[elem, brow, bcol], axis=elem_idx)\n",
    "\n",
    "    CC = te.compute((M, N // bsrR, bsrR, bsrC), bsr_gemm_kernel, name='CC')\n",
    "    k = te.reduce_axis((0, bsrC), name='k')\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(CC[m, n // bsrR, n % bsrR, k], axis=k), name='C')\n",
    "    \n",
    "    def create_bsr_gemm_schedule(C, CC):\n",
    "        s = te.create_schedule(C.op)\n",
    "        md, nd = s[C].op.axis\n",
    "        md1, nd, md2, rd = s[C].tile(md, nd, 8, bsrR)\n",
    "        cd = s[C].op.reduce_axis[0]\n",
    "        s[C].unroll(cd)\n",
    "        s[C].unroll(rd)\n",
    "        s[C].vectorize(md2)\n",
    "        \n",
    "        s[CC].compute_at(s[C], nd)\n",
    "        md2, nd0, rd, cd = s[CC].op.axis\n",
    "        ed = s[CC].op.reduce_axis[0]\n",
    "        s[CC].reorder(nd0, ed, md2, rd, cd)\n",
    "        s[CC].vectorize(cd)\n",
    "        s[CC].unroll(rd)\n",
    "        s[CC].unroll(md2)\n",
    "        return s\n",
    "    \n",
    "    s = create_bsr_gemm_schedule(C, CC)\n",
    "    return s, [Data, Wdat, Wind, Wptr]\n",
    "\n",
    "\n",
    "tr = TVMRunner('bsr_trans_gemm', create_trans_gemm(nhwkkc_data, bsr_1x2))\n",
    "print(tr.lower())\n",
    "tr.time_eval(nhwkkc_data, *unpack_bsr(bsr_1x2), np.float32)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPI Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import topi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=None, workload=('dense_mkl.x86', ('TENSOR', (655360, 576), 'float32'), ('TENSOR', (64, 576), 'float32')). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [655360, 64], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [655360, 576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [64, 576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  attr [0] \"extern_scope\" = 0;\n",
      "  @tir.tvm_call_packed(\"tvm.contrib.mkl.matmul\", @tir.tvm_stack_make_array(A_2, @tir.tvm_stack_make_shape(655360, 576, dtype=handle), 0, 2, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(B_2, @tir.tvm_stack_make_shape(64, 576, dtype=handle), 0, 2, 0f32, 0, dtype=handle), @tir.tvm_stack_make_array(C_2, @tir.tvm_stack_make_shape(655360, 64, dtype=handle), 0, 2, 0f32, 0, dtype=handle), False, True, dtype=int32)\n",
      "}\n",
      "\n",
      "\n",
      "0.5473184809\n"
     ]
    }
   ],
   "source": [
    "def create_topi_dense_mkl(data, weight):\n",
    "    M, K, N, _ = *data.shape, *weight.shape\n",
    "    A = te.placeholder((M, K), name='A')\n",
    "    B = te.placeholder((N, K), name='B')\n",
    "    C = topi.x86.dense_mkl(A, B)\n",
    "    s = topi.x86.schedule_dense_mkl([C])\n",
    "    return s, [A, B, C]\n",
    "\n",
    "\n",
    "tr = TVMRunner('topi_dense_mkl', create_topi_dense_mkl(nhwkkc_data, weight_ohwi_flat))\n",
    "print(tr.lower())\n",
    "tr.time_eval(nhwkkc_data, weight_ohwi_flat, np.float32)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的`sparse_dense_v2`需要给tvm打patch：(v1没有有效schedule）\n",
    "\n",
    "```diff\n",
    "diff --git a/python/tvm/topi/x86/sparse.py b/python/tvm/topi/x86/sparse.py\n",
    "index b629108..6899abc 100644\n",
    "--- a/python/tvm/topi/x86/sparse.py\n",
    "+++ b/python/tvm/topi/x86/sparse.py\n",
    "@@ -34,9 +34,9 @@ def schedule_sparse_dense(outs):\n",
    "             (y_o, y_i) = s[outs[0].op].split(s[outs[0].op].op.axis[1], 2 * simd_widt\n",
    "             s[op].compute_at(s[outs[0]], y_o)\n",
    "             s[outs[0].op].vectorize(y_i)\n",
    "-        if op.tag == \"sparse_dense_bsrmm\":\n",
    "+        if op.tag == \"sparse_dense_bsrmm_v2\":\n",
    "             y_bsrmm = op.input_tensors[0]\n",
    "-            assert y_bsrmm.op.tag == \"sparse_dense_bsrmm_block\"\n",
    "+            assert y_bsrmm.op.tag == \"sparse_dense_bsrmm_block_v2\"\n",
    "             y_reshape = op\n",
    "             (m, num_blocks, b_r) = s[y_bsrmm].op.axis\n",
    "             bs_r = get_const_int(b_r.dom.extent)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(Data_1: handle, Wdat_1: handle, Wind_1: handle, Wptr_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {Wdat: Buffer(Wdat_2: Pointer(float32), float32, [4608, 4, 1], []),\n",
      "             Wptr: Buffer(Wptr_2: Pointer(int32), int32, [17], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [655360, 64], []),\n",
      "             Wind: Buffer(Wind_2: Pointer(int32), int32, [4608], []),\n",
      "             Data: Buffer(Data_2: Pointer(float32), float32, [655360, 576], [])}\n",
      "  buffer_map = {compute_1: compute, Data_1: Data, Wind_1: Wind, Wptr_1: Wptr, Wdat_1: Wdat} {\n",
      "  for (m.n.outer.fused: int32, 0, 10485760) \"parallel\" {\n",
      "    attr [compute_3: Pointer(float32)] \"storage_scope\" = \"global\";\n",
      "    allocate(compute_3, float32x4, [1]) {\n",
      "      compute_3[ramp(0, 1, 4)] = broadcast(0f32, 4)\n",
      "      for (elem_idx: int32, 0, ((int32*)Wptr_2[(floormod(m.n.outer.fused, 16) + 1)] - (int32*)Wptr_2[floormod(m.n.outer.fused, 16)])) {\n",
      "        compute_3[ramp(0, 1, 4)] = ((float32x4*)compute_3[ramp(0, 1, 4)] + ((float32x4*)Wdat_2[ramp((((int32*)Wptr_2[floormod(m.n.outer.fused, 16)]*4) + (elem_idx*4)), 1, 4)]*broadcast((float32*)Data_2[((floordiv(m.n.outer.fused, 16)*576) + (int32*)Wind_2[((int32*)Wptr_2[floormod(m.n.outer.fused, 16)] + elem_idx)])], 4)))\n",
      "      }\n",
      "      compute_2[ramp((m.n.outer.fused*4), 1, 4)] = (float32x4*)compute_3[ramp(0, 1, 4)]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "3.1004867876\n"
     ]
    }
   ],
   "source": [
    "bsr_1x2 = make_bsr_sparse(weight_ohwi_flat, 0.5, (4, 1))\n",
    "\n",
    "def create_topi_sparse_dense_v2(dense, bsr):\n",
    "    M, K, N, _, bsrR, bsrC = *dense.shape, *bsr.shape, *bsr.blocksize\n",
    "    bsrdata, bsrindices, bsrindptr = unpack_bsr(bsr)\n",
    "    Data = te.placeholder(dense.shape, name='Data')\n",
    "    Wdat = te.placeholder(bsrdata.shape, name='Wdat')\n",
    "    Wind = te.placeholder(bsrindices.shape, dtype='int', name='Wind')\n",
    "    Wptr = te.placeholder(bsrindptr.shape, dtype='int', name='Wptr')\n",
    "    Ret = topi.nn.sparse_dense(Data, Wdat, Wind, Wptr, sparse_lhs=False)\n",
    "    s = topi.x86.schedule_sparse_dense([Ret])\n",
    "    return s, [Data, Wdat, Wind, Wptr, Ret]\n",
    "\n",
    "\n",
    "with tvm.target.Target(target):\n",
    "    tr = TVMRunner('topi_sparse_dense_v2', create_topi_sparse_dense_v2(nhwkkc_data, bsr_1x2))\n",
    "    print(tr.lower())\n",
    "    tr.time_eval(nhwkkc_data, *unpack_bsr(bsr_1x2), np.float32)\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(Wdat_1: handle, Wind_1: handle, Wptr_1: handle, Data_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {Wind: Buffer(Wind_2: Pointer(int32), int32, [18432], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [64, 655360], []),\n",
      "             Wdat: Buffer(Wdat_2: Pointer(float32), float32, [18432, 1, 1], []),\n",
      "             Data: Buffer(Data_2: Pointer(float32), float32, [655360, 576], []),\n",
      "             Wptr: Buffer(Wptr_2: Pointer(int32), int32, [65], [])}\n",
      "  buffer_map = {Wdat_1: Wdat, Wptr_1: Wptr, compute_1: compute, Data_1: Data, Wind_1: Wind} {\n",
      "  attr [compute_3: Pointer(float32)] \"storage_scope\" = \"global\";\n",
      "  allocate(compute_3, float32, [41943040]) {\n",
      "    for (nb_j: int32, 0, 64) {\n",
      "      for (i: int32, 0, 655360) {\n",
      "        compute_3[((nb_j*655360) + i)] = 0f32\n",
      "        for (elem_idx: int32, 0, ((int32*)Wptr_2[(nb_j + 1)] - (int32*)Wptr_2[nb_j])) {\n",
      "          compute_3[((nb_j*655360) + i)] = ((float32*)compute_3[((nb_j*655360) + i)] + ((float32*)Wdat_2[((int32*)Wptr_2[nb_j] + elem_idx)]*(float32*)Data_2[((i*576) + (int32*)Wind_2[((int32*)Wptr_2[nb_j] + elem_idx)])]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (m: int32, 0, 64) {\n",
      "      for (n: int32, 0, 655360) {\n",
      "        compute_2[((m*655360) + n)] = (float32*)compute_3[((m*655360) + n)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bsr_2x1 = make_bsr_sparse(weight_ohwi_flat, 0.5, (1, 1))\n",
    "\n",
    "\n",
    "def create_topi_sparse_dense_v1(bsr, dense):\n",
    "    M, K, _, N, bsrR, bsrC = *bsr.shape, *dense.shape, *bsr.blocksize\n",
    "    bsrdata, bsrindices, bsrindptr = unpack_bsr(bsr)\n",
    "    Wdat = te.placeholder(bsrdata.shape, name='Wdat')\n",
    "    Wind = te.placeholder(bsrindices.shape, dtype='int', name='Wind')\n",
    "    Wptr = te.placeholder(bsrindptr.shape, dtype='int', name='Wptr')\n",
    "    Data = te.placeholder(dense.shape, name='Data')\n",
    "    Ret = topi.nn.sparse_dense(Data, Wdat, Wind, Wptr, sparse_lhs=True)\n",
    "    s = topi.x86.schedule_sparse_dense([Ret])\n",
    "    return s, [Wdat, Wind, Wptr, Data, Ret]\n",
    "\n",
    "\n",
    "with tvm.target.Target(target):\n",
    "    tr = TVMRunner('topi_sparse_dense_v1', create_topi_sparse_dense_v1(bsr_2x1, nhwkkc_data))\n",
    "    print(tr.lower())\n",
    "    tr.time_eval(*unpack_bsr(bsr_2x1), nhwkkc_data, np.float32)\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
